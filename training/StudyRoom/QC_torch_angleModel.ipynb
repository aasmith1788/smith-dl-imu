{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from os.path import join\n",
    "from pickle import load\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######### 설정 영역 ########\n",
    "modelVersion = 'Dense_1st_torch'\n",
    "nameDataset = 'IWALQQ_1st'\n",
    "dataType = 'angle' # or moBWHT\n",
    "# 데이터 위치\n",
    "relativeDir = '../../preperation/SAVE_dataSet'\n",
    "dataSetDir = join(relativeDir,nameDataset)\n",
    "# 모델 저장 위치\n",
    "SaveDir = '/restricted/projectnb/movelab/bcha/IMUforKnee/trainedModel/'\n",
    "\n",
    "# tensorboard 위치\n",
    "totalFold = 5\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "lreaningRate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "log_interval = 10\n",
    "############################\n",
    "# 시간 설정\n",
    "time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# CPU or GPU?\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 모델 만들기\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(4242,6000)\n",
    "        self.layer2 = nn.Linear(6000,4000)\n",
    "        self.layer3 = nn.Linear(4000,303)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# 개빠른가..?\n",
    "class Dataset(torch.utils.data.Dataset): \n",
    "  def __init__(self, dataSetDir, dataType, sess, numFold):\n",
    "      self.dataType = dataType # angle or moBWHT\n",
    "      self.sess = sess # train or test\n",
    "      self.load_Data_X = torch.from_numpy(np.load(join(dataSetDir,f\"{str(numFold)}_fold_final_{sess}.npz\"))[f'final_X_{self.sess}'])\n",
    "      self.load_Data_Y = torch.from_numpy(np.load(join(dataSetDir,f\"{str(numFold)}_fold_final_{sess}.npz\"))[f'final_Y_{self.dataType}_{self.sess}'])\n",
    "      self.load_Data_X = torch.squeeze(self.load_Data_X.type(torch.FloatTensor))\n",
    "      self.load_Data_Y = torch.squeeze(self.load_Data_Y.type(torch.FloatTensor))\n",
    "  def __len__(self):\n",
    "      return len(self.load_Data_X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    X = self.load_Data_X[idx]\n",
    "    Y = self.load_Data_Y[idx]\n",
    "    return X, Y\n",
    "\n",
    "def ensure_dir(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "\n",
    "# batch당 총 error를 누적해서 줌\n",
    "# TL = Total Loss per batch\n",
    "def nRMSE_Axis_TLPerbatch(pred, target,axis,load_scaler4Y):\n",
    "    dict_axis = {\n",
    "    'x': 0,\n",
    "    \"y\": 1,\n",
    "    \"z\": 2,\n",
    "    }\n",
    "    axis = dict_axis[axis]\n",
    "    nRMSE_perbatch = 0\n",
    "    # 필요한 sclaer 불러오기\n",
    "    \n",
    "    batchNum = len(target)\n",
    "    for bat in range(batchNum): # batch 내를 순회\n",
    "        pred_axis   = torch.transpose(torch.reshape(torch.squeeze(pred[bat]), [3,-1]), 0, 1)[:,axis]\n",
    "        target_axis = torch.transpose(torch.reshape(torch.squeeze(target[bat]), [3,-1]), 0, 1)[:,axis]\n",
    "        pred_axis = (pred_axis - torch.tensor(load_scaler4Y.min_[axis])) / torch.tensor(load_scaler4Y.scale_[axis])\n",
    "        target_axis = (target_axis - torch.tensor(load_scaler4Y.min_[axis])) / torch.tensor(load_scaler4Y.scale_[axis])\n",
    "        nRMSE = 100 * torch.sqrt(torch.mean(torch.square(pred_axis - target_axis))) / (torch.max(target_axis) - torch.min(target_axis))\n",
    "        nRMSE_perbatch += nRMSE\n",
    "    return nRMSE_perbatch\n",
    "    \n",
    "def ensure_dir(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "layout = {\n",
    "    \"CBD\": {\n",
    "        \"loss(MAE)\": [\"Multiline\", [\"loss(MAE)/training\", \"loss(MAE)/validation\"]],\n",
    "        \"X_nRMSE\": [\"Multiline\", [f'{dataType}_X_nRMSE/training', f\"{dataType}_X_nRMSE/validation\"]],\n",
    "        \"Y_nRMSE\": [\"Multiline\", [f'{dataType}_Y_nRMSE/training', f\"{dataType}_Y_nRMSE/validation\"]],\n",
    "        \"Z_nRMSE\": [\"Multiline\", [f'{dataType}_Z_nRMSE/training', f\"{dataType}_Z_nRMSE/validation\"]],\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:   0%|          | 0/23 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for numFold  in range(totalFold):\n",
    "    print(f'now fold: {numFold}')\n",
    "    # 매 fold마다 새로운 모델\n",
    "    my_model = Mlp()\n",
    "    my_model.to(device)\n",
    "    \n",
    "\n",
    "    # loss function and optimizer define\n",
    "    criterion = nn.L1Loss() # mean absolute error\n",
    "    optimizer = torch.optim.NAdam(my_model.parameters(),lr=lreaningRate)\n",
    "\n",
    "    angle_train = Dataset(dataSetDir, dataType, 'train',numFold)\n",
    "    angle_test  = Dataset(dataSetDir, dataType, 'test', numFold)\n",
    "    train_loader = DataLoader(angle_train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(angle_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 시각화를 위한 tensorboard 초기화\n",
    "    writer = SummaryWriter(f'./logs/pytorch/{time}/{modelVersion}/{nameDataset}/{numFold}_fold')\n",
    "    writer.add_custom_scalars(layout)\n",
    "    x = torch.rand(1, 4242, device=device)\n",
    "    writer.add_graph(my_model,x)\n",
    "    # 학습시작\n",
    "    load_scaler4Y = load(open(join(dataSetDir,f\"{numFold}_fold_scaler4Y_{dataType}.pkl\"), 'rb'))\n",
    "    for epoch in range(epochs):\n",
    "        my_model.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_x_nRMSE = 0\n",
    "        train_y_nRMSE = 0\n",
    "        train_z_nRMSE = 0\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = my_model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * data.size(0) # 이것은 모든 배치의 크기가 일정하지 않을 수 있기 때문에 이렇게 수행함! train_loss는 total loss of batch가 됨\n",
    "            \n",
    "            train_x_nRMSE += nRMSE_Axis_TLPerbatch(output,target, 'x', load_scaler4Y) # 해당 배치에서의 총 loss의 합\n",
    "            dict_axis = {\n",
    "            'x': 0,\n",
    "            \"y\": 1,\n",
    "            \"z\": 2,\n",
    "            }\n",
    "            axis = 'z'\n",
    "            axis = dict_axis[axis]\n",
    "            nRMSE_perbatch = 0\n",
    "            # 필요한 sclaer 불러오기\n",
    "            \n",
    "            batchNum = len(target)\n",
    "            for bat in range(batchNum): # batch 내를 순회\n",
    "                pred_axis   = torch.transpose(torch.reshape(torch.squeeze(output[bat]), [3,-1]), 0, 1)[:,axis]\n",
    "                check_pred_axis = pred_axis\n",
    "                target_axis = torch.transpose(torch.reshape(torch.squeeze(target[bat]), [3,-1]), 0, 1)[:,axis]\n",
    "                pred_axis = (pred_axis - torch.tensor(load_scaler4Y.min_[axis])) / torch.tensor(load_scaler4Y.scale_[axis])\n",
    "                target_axis = (target_axis - torch.tensor(load_scaler4Y.min_[axis])) / torch.tensor(load_scaler4Y.scale_[axis])\n",
    "                nRMSE = 100 * torch.sqrt(torch.mean(torch.square(pred_axis - target_axis))) / (torch.max(target_axis) - torch.min(target_axis))\n",
    "                nRMSE_perbatch += nRMSE\n",
    "                break\n",
    "            break\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0592, -0.0363, -0.0942, -0.0024, -0.0205, -0.1362, -0.0237,  0.1106,\n",
       "         0.0923, -0.0117, -0.0654,  0.0641,  0.0298,  0.0211, -0.1087, -0.0233,\n",
       "        -0.0848,  0.0562,  0.0779,  0.0346, -0.0906, -0.0442,  0.1078,  0.0491,\n",
       "         0.0055,  0.0981, -0.0577, -0.1238, -0.0424, -0.0169,  0.0256, -0.0336,\n",
       "        -0.0697,  0.0233,  0.1024,  0.0869,  0.0695, -0.0568,  0.0842, -0.0024,\n",
       "        -0.0100, -0.0015, -0.0037,  0.0398, -0.0024, -0.0177, -0.0262,  0.0400,\n",
       "         0.0735, -0.0215,  0.1167,  0.0629, -0.0889,  0.1054, -0.0488,  0.0102,\n",
       "         0.0134,  0.0640, -0.0297, -0.0367,  0.1639,  0.0809,  0.1413,  0.0039,\n",
       "        -0.0779, -0.0200,  0.1038, -0.1533, -0.0730, -0.0290, -0.0225, -0.0384,\n",
       "        -0.0351,  0.0494, -0.0738,  0.0561,  0.0312,  0.0980, -0.0529, -0.0670,\n",
       "         0.0809, -0.0676, -0.0455,  0.0523, -0.0259, -0.1318,  0.0432,  0.1121,\n",
       "        -0.1229,  0.0329,  0.0676, -0.0259,  0.0376, -0.0110, -0.0368, -0.0411,\n",
       "        -0.0339,  0.0221,  0.0716,  0.0037,  0.1132,  0.1254,  0.0652,  0.0560,\n",
       "        -0.0501, -0.0279,  0.0097,  0.0703, -0.0193,  0.0306, -0.0478, -0.0837,\n",
       "        -0.0497, -0.0329, -0.0449, -0.0224,  0.0455,  0.0457, -0.0641,  0.0889,\n",
       "         0.0052,  0.0527, -0.0193,  0.0056, -0.0503,  0.0599, -0.0723,  0.1346,\n",
       "        -0.0459, -0.0082, -0.0681,  0.0701, -0.0320, -0.0007,  0.0061,  0.0540,\n",
       "         0.0043, -0.0645,  0.0248,  0.0033,  0.0082, -0.0447,  0.0678, -0.0197,\n",
       "         0.0273, -0.0584, -0.1200, -0.0737,  0.0430,  0.0419, -0.0359, -0.0129,\n",
       "        -0.0080,  0.0887,  0.0218,  0.0385, -0.0073, -0.0248,  0.0520,  0.1038,\n",
       "         0.0044,  0.0708,  0.0906, -0.0224, -0.0183, -0.1124, -0.0327,  0.1145,\n",
       "        -0.0914, -0.0842,  0.0842,  0.0158,  0.0691,  0.0006,  0.0644,  0.0604,\n",
       "         0.1026,  0.1313, -0.0728,  0.0999,  0.0920,  0.0493,  0.1588, -0.0299,\n",
       "         0.0927, -0.0214,  0.0615,  0.0882, -0.0649,  0.0111,  0.0603,  0.0508,\n",
       "        -0.0569,  0.0847, -0.0890,  0.0642,  0.0137, -0.0549,  0.0548, -0.1056,\n",
       "         0.0139, -0.0121, -0.1093, -0.0403, -0.0168,  0.1037, -0.1326,  0.0355,\n",
       "        -0.0253,  0.1287,  0.0159, -0.0025,  0.1412, -0.0042,  0.1476, -0.0181,\n",
       "         0.1707,  0.0145,  0.0283,  0.0569,  0.0240,  0.0639,  0.0211, -0.0272,\n",
       "         0.0938,  0.0456,  0.0052,  0.1568, -0.0759,  0.0395, -0.0056, -0.1240,\n",
       "         0.0708, -0.1736,  0.0805, -0.1164, -0.0665, -0.0064, -0.0715,  0.0151,\n",
       "        -0.0010,  0.0377, -0.0930,  0.0296, -0.0193,  0.0102,  0.0751,  0.0021,\n",
       "         0.0857,  0.0393, -0.0151,  0.0446, -0.0877, -0.0287, -0.0412, -0.0492,\n",
       "         0.1107,  0.0836, -0.0277,  0.0219,  0.0446, -0.1755,  0.0089,  0.0932,\n",
       "        -0.0237, -0.0030,  0.1073, -0.0420, -0.0406,  0.0816,  0.0349, -0.0021,\n",
       "        -0.0930, -0.0628, -0.0032,  0.0289, -0.0450, -0.1072, -0.0020, -0.0546,\n",
       "        -0.0608, -0.0053, -0.1024, -0.0210, -0.0108, -0.0780, -0.0055,  0.0216,\n",
       "        -0.0414, -0.0285,  0.0702,  0.0153,  0.0353,  0.0760, -0.0552, -0.1317,\n",
       "        -0.0077,  0.0529, -0.0773, -0.1022,  0.0182,  0.0792, -0.0099],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[bat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1093, -0.0403, -0.0168,  0.1037, -0.1326,  0.0355, -0.0253,  0.1287,\n",
       "         0.0159, -0.0025,  0.1412, -0.0042,  0.1476, -0.0181,  0.1707,  0.0145,\n",
       "         0.0283,  0.0569,  0.0240,  0.0639,  0.0211, -0.0272,  0.0938,  0.0456,\n",
       "         0.0052,  0.1568, -0.0759,  0.0395, -0.0056, -0.1240,  0.0708, -0.1736,\n",
       "         0.0805, -0.1164, -0.0665, -0.0064, -0.0715,  0.0151, -0.0010,  0.0377,\n",
       "        -0.0930,  0.0296, -0.0193,  0.0102,  0.0751,  0.0021,  0.0857,  0.0393,\n",
       "        -0.0151,  0.0446, -0.0877, -0.0287, -0.0412, -0.0492,  0.1107,  0.0836,\n",
       "        -0.0277,  0.0219,  0.0446, -0.1755,  0.0089,  0.0932, -0.0237, -0.0030,\n",
       "         0.1073, -0.0420, -0.0406,  0.0816,  0.0349, -0.0021, -0.0930, -0.0628,\n",
       "        -0.0032,  0.0289, -0.0450, -0.1072, -0.0020, -0.0546, -0.0608, -0.0053,\n",
       "        -0.1024, -0.0210, -0.0108, -0.0780, -0.0055,  0.0216, -0.0414, -0.0285,\n",
       "         0.0702,  0.0153,  0.0353,  0.0760, -0.0552, -0.1317, -0.0077,  0.0529,\n",
       "        -0.0773, -0.1022,  0.0182,  0.0792, -0.0099],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_pred_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        train_loss /= len(train_loader.sampler) \n",
    "        train_x_nRMSE /= len(train_loader.sampler) \n",
    "        train_y_nRMSE /= len(train_loader.sampler) \n",
    "        train_z_nRMSE /= len(train_loader.sampler) \n",
    "        writer.add_scalar('loss(MAE)/training', train_loss, epoch)\n",
    "        writer.add_scalar(f'{dataType}_X_nRMSE/training', train_x_nRMSE, epoch)\n",
    "        writer.add_scalar(f'{dataType}_Y_nRMSE/training', train_y_nRMSE, epoch)\n",
    "        writer.add_scalar(f'{dataType}_Z_nRMSE/training', train_z_nRMSE, epoch)\n",
    "\n",
    "        test_loss = 0\n",
    "        test_x_nRMSE = 0\n",
    "        test_y_nRMSE = 0\n",
    "        test_z_nRMSE = 0\n",
    "        my_model.eval()  # batch norm이나 dropout 등을 train mode 변환\n",
    "        with torch.no_grad():  # autograd engine, 즉 backpropagatin이나 gradient 계산 등을 꺼서 memory usage를 줄이고 속도를 높임\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = my_model(data)\n",
    "                loss = criterion(output,target)\n",
    "                test_loss += loss.item() * data.size(0)\n",
    "                test_x_nRMSE += nRMSE_Axis_TLPerbatch(output,target, 'x', load_scaler4Y)# 해당 배치에서의 총 loss의 합\n",
    "                test_y_nRMSE += nRMSE_Axis_TLPerbatch(output,target, 'y', load_scaler4Y) # 해당 배치에서의 총 loss의 합\n",
    "                test_z_nRMSE += nRMSE_Axis_TLPerbatch(output,target, 'z', load_scaler4Y) # 해당 배치에서의 총 loss의 합             \n",
    "\n",
    "            test_loss /= len(test_loader.sampler)\n",
    "            test_x_nRMSE /= len(test_loader.sampler) \n",
    "            test_y_nRMSE /= len(test_loader.sampler) \n",
    "            test_z_nRMSE /= len(test_loader.sampler)\n",
    "            writer.add_scalar('loss(MAE)/validation', test_loss, epoch)\n",
    "            writer.add_scalar(f'{dataType}_X_nRMSE/validation', test_x_nRMSE, epoch)\n",
    "            writer.add_scalar(f'{dataType}_Y_nRMSE/validation', test_y_nRMSE, epoch)\n",
    "            writer.add_scalar(f'{dataType}_Z_nRMSE/validation', test_z_nRMSE, epoch)\n",
    "        print(f'\\nTrain set: Average loss: {train_loss:.4f}, X_nRMSE: {train_x_nRMSE}, Y_nRMSE: {train_y_nRMSE}, Z_nRMSE: {train_z_nRMSE}'\n",
    "             +f'\\nTest set: Average loss: {test_loss:.4f}, X_nRMSE: {test_x_nRMSE}, Y_nRMSE: {test_y_nRMSE}, Z_nRMSE: {test_z_nRMSE}')\n",
    "    writer.close()\n",
    "    dir_save_torch = join(SaveDir,modelVersion,nameDataset)\n",
    "    ensure_dir(dir_save_torch)\n",
    "    model_scripted = torch.jit.script(my_model) # Export to TorchScript\n",
    "    model_scripted.save(join(dir_save_torch,f'{dataType}_{numFold}_fold.pt')) # Save\n",
    "    # 저장된 모델 불러올 때\n",
    "    # 항상 불러온 모델 뒤에 model.eval() 붙일 것!\n",
    "    # https://tutorials.pytorch.kr/beginner/saving_loading_models.html#export-load-model-in-torchscript-format\n",
    "    # model = torch.jit.load('model_scripted.pt')\n",
    "    # model.eval()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8027b976bef41ecb4ff26e7656eddf5218dd43e3122c2750571b73252d337b25"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('torchIMU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
