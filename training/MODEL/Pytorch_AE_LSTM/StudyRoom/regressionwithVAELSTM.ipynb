{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설정\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.testing import make_tensor\n",
    "from pytorch_model_summary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "from CBDtorch.vaelstm import *\n",
    "from CBDtorch.custom import Dataset4autoencoder\n",
    "from CBDtorch.dirs import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일단 확인용으로 VAELSTM 만들기\n",
    "dataType = 'angle'\n",
    "numFold = 0\n",
    "seq_len = 101\n",
    "num_features = 42\n",
    "embedding_dim = 30 \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "my_model = VariationalEncoder(seq_len, num_features, embedding_dim, device)\n",
    "filename = f'{dataType}_{embedding_dim}_{numFold}_fold'\n",
    "filename = join('.',filename)\n",
    "torch.save(my_model.state_dict(), join('.',filename)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메인 아이디어\n",
    "class regressor(nn.Module):\n",
    "    def __init__(self,filename,emb_dims,*args):\n",
    "        super().__init__()\n",
    "        self.VAE = VariationalEncoder(*args)\n",
    "        self.VAE.load_state_dict(torch.load(filename))\n",
    "        self.dense = nn.Sequential(nn.Linear(emb_dims,2048),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(p=0.5),  # 노드를 학습과정에서 얼만큼 활용 안할지\n",
    "                                        nn.Linear(2048,2048),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(p=0.5),\n",
    "                                        nn.Linear(2048,1024),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(p=0.5),\n",
    "                                        nn.Linear(1024,512),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(p=0.2),\n",
    "                                        nn.Linear(512,512),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(p=0.2),\n",
    "                                        nn.Linear(512,303),\n",
    "                                        )\n",
    "        ##You can use as many linear layers and other activations as you want\n",
    "    def forward(self, x):\n",
    "        x = self.VAE(x)\n",
    "        output = self.dense(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "reg = regressor(filename, embedding_dim, seq_len, num_features, embedding_dim, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "           Layer (type)         Input Shape         Param #     Tr. Param #\n",
      "============================================================================\n",
      "   VariationalEncoder-1       [32, 101, 42]          90,060          90,060\n",
      "               Linear-2            [32, 30]          63,488          63,488\n",
      "                 ReLU-3          [32, 2048]               0               0\n",
      "              Dropout-4          [32, 2048]               0               0\n",
      "               Linear-5          [32, 2048]       4,196,352       4,196,352\n",
      "                 ReLU-6          [32, 2048]               0               0\n",
      "              Dropout-7          [32, 2048]               0               0\n",
      "               Linear-8          [32, 2048]       2,098,176       2,098,176\n",
      "                 ReLU-9          [32, 1024]               0               0\n",
      "             Dropout-10          [32, 1024]               0               0\n",
      "              Linear-11          [32, 1024]         524,800         524,800\n",
      "                ReLU-12           [32, 512]               0               0\n",
      "             Dropout-13           [32, 512]               0               0\n",
      "              Linear-14           [32, 512]         262,656         262,656\n",
      "                ReLU-15           [32, 512]               0               0\n",
      "             Dropout-16           [32, 512]               0               0\n",
      "              Linear-17           [32, 512]         155,439         155,439\n",
      "============================================================================\n",
      "Total params: 7,390,971\n",
      "Trainable params: 7,390,971\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "           Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "============================================================================\n",
      "   VariationalEncoder-1            [32, 30]          90,060          90,060\n",
      "               Linear-2          [32, 2048]          63,488          63,488\n",
      "                 ReLU-3          [32, 2048]               0               0\n",
      "              Dropout-4          [32, 2048]               0               0\n",
      "               Linear-5          [32, 2048]       4,196,352       4,196,352\n",
      "                 ReLU-6          [32, 2048]               0               0\n",
      "              Dropout-7          [32, 2048]               0               0\n",
      "               Linear-8          [32, 1024]       2,098,176       2,098,176\n",
      "                 ReLU-9          [32, 1024]               0               0\n",
      "             Dropout-10          [32, 1024]               0               0\n",
      "              Linear-11           [32, 512]         524,800         524,800\n",
      "                ReLU-12           [32, 512]               0               0\n",
      "             Dropout-13           [32, 512]               0               0\n",
      "              Linear-14           [32, 512]         262,656         262,656\n",
      "                ReLU-15           [32, 512]               0               0\n",
      "             Dropout-16           [32, 512]               0               0\n",
      "              Linear-17           [32, 303]         155,439         155,439\n",
      "============================================================================\n",
      "Total params: 7,390,971\n",
      "Trainable params: 7,390,971\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# show input shape\n",
    "print(summary(reg, torch.zeros(32, 101, 42), show_input=True))\n",
    "# show output shape\n",
    "print(summary(reg, torch.zeros(32, 101, 42), show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mz:\\PROJECTS\\iwalqq\\Data\\V3D\\Output\\IMU Deep Learning\\IMUforKnee\\training\\MODEL\\Pytorch_AE_LSTM\\StudyRoom\\regressionwithVAELSTM.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/z%3A/PROJECTS/iwalqq/Data/V3D/Output/IMU%20Deep%20Learning/IMUforKnee/training/MODEL/Pytorch_AE_LSTM/StudyRoom/regressionwithVAELSTM.ipynb#ch0000006?line=0'>1</a>\u001b[0m nn\u001b[39m.\u001b[39;49mL1Loss()(\u001b[39m10\u001b[39;49m,\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\bcha\\Miniconda3\\envs\\imu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bcha\\Miniconda3\\envs\\imu\\lib\\site-packages\\torch\\nn\\modules\\loss.py:96\u001b[0m, in \u001b[0;36mL1Loss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/modules/loss.py?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/modules/loss.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49ml1_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\bcha\\Miniconda3\\envs\\imu\\lib\\site-packages\\torch\\nn\\functional.py:3220\u001b[0m, in \u001b[0;36ml1_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/functional.py?line=3215'>3216</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, target):\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/functional.py?line=3216'>3217</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/functional.py?line=3217'>3218</a>\u001b[0m         l1_loss, (\u001b[39minput\u001b[39m, target), \u001b[39minput\u001b[39m, target, size_average\u001b[39m=\u001b[39msize_average, reduce\u001b[39m=\u001b[39mreduce, reduction\u001b[39m=\u001b[39mreduction\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/functional.py?line=3218'>3219</a>\u001b[0m     )\n\u001b[1;32m-> <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/functional.py?line=3219'>3220</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39;49msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/functional.py?line=3220'>3221</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/functional.py?line=3221'>3222</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m). \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/functional.py?line=3222'>3223</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/functional.py?line=3223'>3224</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()),\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/functional.py?line=3224'>3225</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/functional.py?line=3225'>3226</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/bcha/Miniconda3/envs/imu/lib/site-packages/torch/nn/functional.py?line=3226'>3227</a>\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "nn.L1Loss()(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13b808b5077e789f7dde84b4147c1c98f2537031315824d3f8153389e6a3e631"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('imu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
