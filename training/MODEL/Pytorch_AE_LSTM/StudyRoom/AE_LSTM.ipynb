{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설정\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "from pickle import load\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 설정 영역 ########\n",
    "# 실험 관련 세팅\n",
    "exp_name = 'torch_20220511' # 실험 이름 혹은 오늘 날짜\n",
    "modelVersion = 'Dense_1st_torch'\n",
    "nameDataset = 'IWALQQ_AE_1st'\n",
    "dataType = 'angle' # or moBWHT\n",
    "\n",
    "#################################\n",
    "# 여기는 grid로 돌림!\n",
    "#################################\n",
    "list_learningRate = {0:0.006, 1:0.008, 2:0.01} # opt1 \n",
    "list_batch_size = {0:128} # opt2\n",
    "list_lossFunction =  {0:\"MAE\"} # opt2\n",
    "\n",
    "totalFold = 5 # total fold, I did 5-fold cross validation\n",
    "epochs = 1000 # total epoch \n",
    "log_interval = 10 # frequency for saving log file\n",
    "count = 0 # In SCC, get count for grid-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내 모델을 구현하기 위한 세부 sub module\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.seq_len, self.n_features = seq_len, n_features\n",
    "        self.embedding_dim, self.hidden_dim = (\n",
    "            embedding_dim, 2 * embedding_dim\n",
    "        )\n",
    "        self.rnn1 = nn.LSTM(\n",
    "          input_size=n_features,\n",
    "          hidden_size=self.hidden_dim,\n",
    "          num_layers=1,\n",
    "          batch_first=True,\n",
    "          bidirectional=True\n",
    "        )\n",
    "        self.rnn2 = nn.LSTM(\n",
    "          input_size=self.hidden_dim * 2,\n",
    "          hidden_size=embedding_dim,\n",
    "          num_layers=1,\n",
    "          batch_first=True,\n",
    "          bidirectional=True\n",
    "        )\n",
    "        self.output_layer = torch.nn.Linear(self.embedding_dim * 2, self.embedding_dim) # bidirectianl이 켜져 있어서 그럼\n",
    "      \n",
    "    def forward(self, x):\n",
    "        x, (_, _) = self.rnn1(x)\n",
    "        x, (hidden_n, _) = self.rnn2(x)\n",
    "        return  self.output_layer(x[:,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show input shape\n",
    "print(summary(Encoder(101, 42, 30), torch.zeros((32, 101, 42)), show_input=True))\n",
    "# show output shape\n",
    "print(summary(Encoder(101, 42, 30), torch.zeros((32, 101, 42)), show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/any-pytorch-function-can-work-as-keras-timedistributed/1346/25\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        t, n = x.size(0), x.size(1)\n",
    "        x_reshape = x.contiguous().view(t * n, -1)  # (samples * timesteps, input_size)\n",
    "        y = self.module(x_reshape)\n",
    "        # We have to reshape Y\n",
    "        y = y.contiguous().view(t, n, -1)  # (samples, timesteps, output_size)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, seq_len, input_dim, n_features):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.seq_len, self.input_dim = seq_len, input_dim\n",
    "        self.hidden_dim, self.n_features = 2 * input_dim, n_features\n",
    "        self.rnn1 = nn.LSTM(\n",
    "          input_size=input_dim,\n",
    "          hidden_size=input_dim,\n",
    "          num_layers=1,\n",
    "          batch_first=True,\n",
    "          bidirectional = True\n",
    "        )\n",
    "        self.rnn2 = nn.LSTM(\n",
    "          input_size=input_dim * 2,\n",
    "          hidden_size=self.hidden_dim,\n",
    "          num_layers=1,\n",
    "          batch_first=True,\n",
    "          bidirectional = True\n",
    "        )\n",
    "        self.output_layer = torch.nn.Linear(self.hidden_dim * 2, self.n_features)\n",
    "        self.timedist = TimeDistributed(self.output_layer)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(f'decoder first shape of x: {x.shape}')\n",
    "        x = x.reshape(-1,1,self.input_dim).repeat(1,self.seq_len,1)\n",
    "        # print(f'decoder after repeatvector shape of x: {x.shape}')       \n",
    "        x, (hidden_n, cell_n) = self.rnn1(x)\n",
    "        x, (hidden_n, cell_n) = self.rnn2(x)\n",
    "        # print(f'decoder last shape of x: {self.timedist(x).shape}')\n",
    "        return self.timedist(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show input shape\n",
    "print(summary(Decoder(101, 20, 42), torch.zeros((20)), show_input=True))\n",
    "# show output shape\n",
    "print(summary(Decoder(101, 20, 42), torch.zeros((20)), show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main module\n",
    "class RecurrentAutoencoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=30):\n",
    "        super(RecurrentAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder(seq_len, n_features, embedding_dim)#.to(device)\n",
    "        self.decoder = Decoder(seq_len, embedding_dim, n_features)#.to(device)\n",
    "    def forward(self, x):\n",
    "        # print(f'first shape of x: {x.shape}')\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        # print(f'last shape of x: {x.shape}')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# show input shape\n",
    "print(summary(RecurrentAutoencoder(101, 42, 30), torch.zeros((16, 101, 42)), show_input=True))\n",
    "\n",
    "# show output shape\n",
    "print(summary(RecurrentAutoencoder(101, 42, 30), torch.zeros((16, 101, 42)), show_input=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 루프 함수화 (구현)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습루프 구현하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 루프 함수화 (예제)\n",
    "- 참고 https://koreapy.tistory.com/739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, device): \n",
    "    size = len(dataloader.dataset) \n",
    "    for batch, (X, y) in enumerate(dataloader): \n",
    "        X, y = X.to(device), y.to(device) \n",
    "        # Compute prediction error \n",
    "        pred = model(X) \n",
    "        loss = loss_fn(pred, y) \n",
    "        # Backpropagation \n",
    "        optimizer.zero_grad() \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        if batch % 100 == 0: \n",
    "            loss, current = loss.item(), batch * len(X) \n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, device): \n",
    "    size = len(dataloader.dataset) \n",
    "    num_batches = len(dataloader) \n",
    "    model.eval() \n",
    "    test_loss, correct = 0, 0 \n",
    "    with torch.no_grad(): \n",
    "        for X, y in dataloader: X, y = X.to(device), y.to(device) \n",
    "        pred = model(X) \n",
    "        test_loss += loss_fn(pred, y).item() \n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item() \n",
    "        test_loss /= num_batches \n",
    "        correct /= size \n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5 \n",
    "for t in range(epochs): \n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn) \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 배운것\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 3, 4)\n",
    "print(a.size())\n",
    "print(a.stride())\n",
    "print(a.is_contiguous())\n",
    "a = a.transpose(0, 1)\n",
    "print(a.is_contiguous())\n",
    "a = a.contiguous()\n",
    "a = a.view(-1)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "658a9c5b3d323e3210e486fb511d93c908db110777364f82495071cb58f332a9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('imu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
