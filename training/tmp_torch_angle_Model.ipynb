{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch를 이용한 angle 추정 모델 - dense_2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from os.path import join\n",
    "from pickle import load\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 설정 영역 ########\n",
    "modelVersion = 'Dense_1st'\n",
    "nameDataset = 'IWALQQ_2nd'\n",
    "goal = 'moment_BWHT'\n",
    "# 데이터 위치\n",
    "relativeDir = '../preperation/SAVE_dataSet'\n",
    "dataSetDir = join(relativeDir,nameDataset)\n",
    "# tensorboard 위치\n",
    "totalFold = 5\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "lreaningRate = 0.002\n",
    "batch_size = 32\n",
    "\n",
    "log_interval = 10\n",
    "############################\n",
    "# 시간 설정\n",
    "time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# CPU or GPU?\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "# 모델 만들기\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(4242,6000)\n",
    "        self.layer2 = nn.Linear(6000,4000)\n",
    "        self.layer3 = nn.Linear(4000,303)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset): \n",
    "  def __init__(self, dataSetDir, dataType, sess, numFold):\n",
    "      self.dataType = dataType # angle or moBWHT\n",
    "      self.sess = sess # train or test\n",
    "      self.load_Data = np.load(join(dataSetDir,f\"{str(numFold)}_fold_final_{sess}.npz\"))\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.load_Data[f'final_X_{self.sess}'])\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    X = torch.squeeze(torch.FloatTensor(self.load_Data[f'final_X_{self.sess}'][idx]))\n",
    "    Y = torch.squeeze(torch.FloatTensor(self.load_Data[f'final_Y_{self.dataType}_{self.sess}'][idx]))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now fold: 0\n",
      "Train Epoch: 0 [0/702 (0%)]\tLoss: 0.513098\n",
      "Train Epoch: 0 [160/702 (23%)]\tLoss: 0.479317\n",
      "Train Epoch: 0 [320/702 (45%)]\tLoss: 0.301148\n",
      "Train Epoch: 0 [480/702 (68%)]\tLoss: 0.280182\n",
      "Train Epoch: 0 [640/702 (91%)]\tLoss: 0.460834\n",
      "\n",
      "Train set: Average loss: 4.4149, Test set: Average loss: 1.8495\n"
     ]
    }
   ],
   "source": [
    "# load data from .npz\n",
    "for numFold  in range(totalFold):\n",
    "    print(f'now fold: {numFold}')\n",
    "    # 매 fold마다 새로운 모델\n",
    "    my_model = Mlp()\n",
    "    my_model.to(DEVICE)\n",
    "    # loss function and optimizer define\n",
    "    criterion = nn.L1Loss() # mean absolute error\n",
    "    optimizer = torch.optim.NAdam(my_model.parameters(),lr=lreaningRate)\n",
    "\n",
    "    angle_train = Dataset(dataSetDir, 'angle', 'train',numFold)\n",
    "    angle_test  = Dataset(dataSetDir, 'angle', 'test', numFold)\n",
    "    train_loader = DataLoader(angle_train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(angle_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 시각화를 위한 tensorboard 초기화\n",
    "    writer = SummaryWriter(f'./logs/pytorch/{modelVersion}/{nameDataset}/{numFold}_fold')\n",
    "    x = torch.rand(1,4242, device=DEVICE)\n",
    "    writer.add_graph(my_model,x)\n",
    "    # 학습시작\n",
    "    for epoch in range(epochs):\n",
    "        my_model.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = my_model(data)\n",
    "            loss = criterion(output, target)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "            \n",
    "        train_loss /= len(train_loader) \n",
    "        writer.add_scalar('training loss(MAE)', train_loss, epoch)\n",
    "\n",
    "\n",
    "        my_model.eval()  # batch norm이나 dropout 등을 train mode 변환\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        with torch.no_grad():  # autograd engine, 즉 backpropagatin이나 gradient 계산 등을 꺼서 memory usage를 줄이고 속도를 높임\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "                output = my_model(data)\n",
    "                test_loss += criterion(output,target).item()                \n",
    "\n",
    "            test_loss /= len(test_loader)\n",
    "            writer.add_scalar('validation loss(MAE)', test_loss, epoch)\n",
    "        print(f'\\nTrain set: Average loss: {train_loss:.4f}, Test set: Average loss: {test_loss:.4f}')\n",
    "        break\n",
    "    writer.close()\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이번에 배운 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    \"\"\"We create neural nets by subclassing the torch.nn.Module class in\n",
    "    the newly defined class, we define 2 things:\n",
    "    1) The network elements/layers; these are defined in the __init__ method\n",
    "    2) The network behavior! What happens when we call our model on an input\n",
    "    (here we call the input 'x')\n",
    "\n",
    "    Our model is thus composed of 2 Conv and 2 Linear layers, each with a\n",
    "    different size. When we call our model against an input example, we compute\n",
    "    the output from each layer and in between we apply the ReLU function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = torch.nn.Conv2d(in_channels=1, out_channels=2, kernel_size=5)\n",
    "        self.layer2 = torch.nn.Conv2d(in_channels=2, out_channels=2, kernel_size=5)\n",
    "        self.layer3 = torch.nn.Linear(800, 16)\n",
    "        self.layer4 = torch.nn.Linear(16, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass through conv layers\n",
    "        x = self.layer1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "\n",
    "        # pass through linear layers\n",
    "        x = torch.flatten(x, start_dim=1)  # flatten the output of convolution\n",
    "        x = self.layer3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# we initialize our model as thus:\n",
    "# Congrats! You just built a neural network with PyTorch :-)\n",
    "my_model = Net()\n",
    "\n",
    "\n",
    "# GPU-aware programming\n",
    "\"\"\"\n",
    "our PyTorch module loads automatically to CPU, and afterwards we can decide to\n",
    "send it to GPU using .to() method. In fact tensor.to() method can send any\n",
    "PyTorch tensor to any device, not just models.\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(DEVICE)\n",
    "my_model.to(DEVICE)  # this sends the model to the appropriate device\n",
    "\n",
    "# Get data\n",
    "DATA_PATH = \"data/FashionMNIST/\"\n",
    "# transforms\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "train_ds = torchvision.datasets.FashionMNIST(\n",
    "    root=DATA_PATH, train=True, transform=transform, download=True\n",
    ")\n",
    "test_ds = torchvision.datasets.FashionMNIST(\n",
    "    root=DATA_PATH, train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "\"\"\"Let's define our training data loader. We will adopt a batch size of 16.\n",
    "Shuffling the data is also useful for training. 0 workers means that the data\n",
    "will be loaded in the main process.\n",
    "\"\"\"\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=16, shuffle=True, num_workers=0\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_ds, batch_size=16, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "# Let's create a translation from the class numbers to Human-Readable\n",
    "# (and meaningful) text labels\n",
    "\n",
    "LABEL_DICT = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\",\n",
    "}\n",
    "\n",
    "\"\"\"There are many Optimizers in the PyTorch Library. We select the ADAM\n",
    "optimizer as one of the most recognizable and efficient optimizers in the\n",
    "Deep Learning field. We feed the optimizer object, the parameters which it\n",
    "will optimize!\n",
    "\"\"\"\n",
    "optimizer = torch.optim.Adam(my_model.parameters())\n",
    "\n",
    "\"\"\"There are many Loss functions in the PyTorch Library. We pick one that is\n",
    "suitable for the problem we are working on (Image Classification). It is called\n",
    "Cross Entropy Loss.\n",
    "\"\"\"\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\"\"\"We leverage the classification_report function in Sci-Kit Learn! More info\n",
    "here: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "SK Learn has many other metrics, you are welcome to check them out and use as\n",
    "needed for your problem.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def compute_metrics(labels, preds, mode=\"train\"):\n",
    "    \"\"\"print out standard metrics for classification.\"\"\"\n",
    "    # we use the values in the dictionary we defined earlier (see above)\n",
    "    # to define a list of class names\n",
    "    # this is helpful in making the report more meaningful!\n",
    "    class_names = list(LABEL_DICT.values())\n",
    "    labels = torch.cat(labels)  # concatenate list into tensor\n",
    "    preds = torch.cat(preds)\n",
    "    metrics = classification_report(\n",
    "        y_true=labels,\n",
    "        y_pred=preds,\n",
    "        target_names=class_names,\n",
    "        output_dict=True,\n",
    "    )\n",
    "    print(f\"\\n-------- begin report: {mode} ----------------------------\\n\")\n",
    "    print(f\"prediction sample: {preds[0:10]}, label sample: {labels[0:10]}\")\n",
    "    for key, value in metrics.items():\n",
    "        print(key, value)\n",
    "    print(f\"\\n-------- end report: {mode} -------------------------------\\n\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_step(my_model, images, labels, optimizer, criterion):\n",
    "    # zero out the gradient parameters\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "\n",
    "    # forward step: compute model output\n",
    "    prediction = my_model(images)\n",
    "\n",
    "    # backward step: compute loss\n",
    "    loss = criterion(prediction, labels)\n",
    "    loss.backward()  # calculate mini-batch grads\n",
    "\n",
    "    # optimizer step: update the model parameters\n",
    "    optimizer.step()  # update weights afters mini-batch\n",
    "    return prediction, loss\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 5  # how many times will we go over our data?\n",
    "\n",
    "\"\"\"Some Notes:\n",
    "1) tqdm is a helpful tool that provides nice \"progress bar\" graphic in the\n",
    "output console :-) our work now looks professional\n",
    "2) Models have 2 modes, training and evaluation. Since we're going to train\n",
    "our model, we change the mode to train\n",
    "\"\"\"\n",
    "my_model.train()  # change model from eval mode to train mode\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0.0  # reset epoch loss\n",
    "    all_preds = []  # list of all predictions in the epoch\n",
    "    all_labels = []  # list of all labels in the epoch\n",
    "    num_batches = len(train_loader)\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "        # get data\n",
    "        images, labels = batch\n",
    "        images = images.to(device=DEVICE, dtype=torch.float)\n",
    "        labels = labels.to(device=DEVICE, dtype=torch.long)\n",
    "        # we defined \"device\" when we defined the model, see above\n",
    "\n",
    "        prediction, loss = train_step(my_model, images, labels, optimizer, criterion)\n",
    "        epoch_loss += loss  # compile batch loss\n",
    "\n",
    "        # metrics\n",
    "        \"\"\"Get the classification using the argmax function and send the tensor\n",
    "        to CPU using .cpu() (or do nothing if they're already in CPU). This is\n",
    "        important, our compute_metrics function expects CPU tensors\n",
    "        \"\"\"\n",
    "        preds = torch.argmax(prediction, dim=1).cpu()\n",
    "        labels = labels.cpu()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "        # batch ends\n",
    "\n",
    "    # epoch ends\n",
    "    epoch_loss /= num_batches\n",
    "    print(\"==================== train ==============\")\n",
    "    print(f\"\\n Train Loss: {epoch_loss} \\n\")\n",
    "\n",
    "    # Compute metrics for classification\n",
    "    metrics = compute_metrics(all_labels, all_preds)\n",
    "print(\"finished training!\")\n",
    "# Congrats! you just trained your first neural network in PyTorch\n",
    "\n",
    "\n",
    "# evaluation\n",
    "my_model.eval()\n",
    "with torch.no_grad():\n",
    "    val_loss = 0.0  # reset epoch loss\n",
    "    all_preds = []  # list of all predictions in the epoch\n",
    "    all_labels = []  # list of all labels in the epoch\n",
    "    num_batches = len(test_loader)\n",
    "    for batch in tqdm(test_loader, desc=f\"validation progress\"):\n",
    "        # get data\n",
    "        images, labels = batch\n",
    "        images = images.to(device=DEVICE, dtype=torch.float)\n",
    "        labels = labels.to(device=DEVICE, dtype=torch.long)\n",
    "        # we defined \"device\" when we defined the model, see above\n",
    "\n",
    "        prediction = my_model(images)\n",
    "        loss = criterion(prediction, labels)\n",
    "        val_loss += loss  # compile batch loss\n",
    "\n",
    "        # metrics\n",
    "        preds = torch.argmax(prediction, dim=1).cpu()\n",
    "        labels = labels.cpu()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "        # batch ends\n",
    "\n",
    "print(\"==================== evaluation ==============\")\n",
    "print(\"\\n Validation Loss: %f \\n\" % val_loss)\n",
    "\n",
    "metrics = compute_metrics(all_labels, all_preds, mode=\"val\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9017f964ece69a1881201e9842c94c23e00b0e3c9573699033b2bdebcff634b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('sccIMU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
