{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat after Normalization\n",
    "### recommended!\n",
    "# Only by doing this can features be preserved without loss and keep the data intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import savez_compressed\n",
    "from numpy import load\n",
    "from natsort import natsorted\n",
    "import os\n",
    "!pip install scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler ,RobustScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from os.path import join\n",
    "\n",
    "from pickle import dump, load\n",
    "\n",
    "seed_rand = 41 # 2nd 777 # 1st 41\n",
    "nameDataset = \"IWALQQ_1st_correction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeColumnsWOMAG():\n",
    "    \"\"\"\n",
    "    Creates column names for biomechanical data excluding magnetometer (MAG) sensors.\n",
    "    Returns standardized column names for IMU and biomech without MAG columns.\n",
    "    \"\"\"\n",
    "    SIDEIDX = ['non','oa']  # Non-operated vs operated leg sides\n",
    "    PARTIDX = ['shank','shoe','thigh']  # Body parts where sensors are placed\n",
    "    TYPEIDX = ['ACC','GYRO','MAG']  # Sensor types: Accelerometer, Gyroscope, Magnetometer\n",
    "    AXISIDX = ['X', 'Y', 'Z']  # 3D coordinate axes\n",
    "    \n",
    "    # Generate leg IMU column names (54 columns total)\n",
    "    LEGCOLUMNSLENGTH = 54\n",
    "    COl_imu_legs = [f'{SIDEIDX[int(i//(LEGCOLUMNSLENGTH/2))]}\\\n",
    "_{PARTIDX[(i//(len(TYPEIDX)*len(AXISIDX)))%len(PARTIDX)]}\\\n",
    "_{TYPEIDX[(i//(len(AXISIDX)))%len(TYPEIDX)]}\\\n",
    "_{AXISIDX[i%len(AXISIDX)]}' for i in range(0,LEGCOLUMNSLENGTH)]\n",
    "    \n",
    "    # Generate trunk IMU column names (9 columns total)\n",
    "    TRKCOLUMNSLENGTH = 9\n",
    "    Col_imu_trunk = [f'trunk_{TYPEIDX[(i//(len(AXISIDX)))%len(TYPEIDX)]}_{AXISIDX[i%len(AXISIDX)]}' for i in range(0,TRKCOLUMNSLENGTH)]\n",
    "    \n",
    "    # Generate biomeche column names (12 columns total)\n",
    "    FPCOLUMNSLENGTH = 12\n",
    "    FPTYPEIDX = ['GRF','ANGLE','MONM','MOBWHT']  # Ground Reaction Force, Angle, Moment, Mobile Weight\n",
    "    Col_FP = [f'{FPTYPEIDX[(i//(len(AXISIDX)))%len(FPTYPEIDX)]}_{AXISIDX[i%len(AXISIDX)]}' for i in range(0,FPCOLUMNSLENGTH)]\n",
    "    \n",
    "    # Combine all column names\n",
    "    newColumns = COl_imu_legs+Col_imu_trunk+Col_FP\n",
    "    # Filter out magnetometer columns for final dataset\n",
    "    newColumnswithoutMAG = [col for col in newColumns if not 'MAG' in col] \n",
    "    return newColumnswithoutMAG\n",
    "\n",
    "# Convert subject-based classification back to overall data index basis\n",
    "# Function to convert subject-based KFold splits into subject fold indices\n",
    "def kfold2subfold(arrName,listData,train,test):\n",
    "    \"\"\"\n",
    "    Converts subject-based train/test splits to data index-based splits.\n",
    "    \n",
    "    Args:\n",
    "        arrName: Array of subject IDs for train/test splits\n",
    "        listData: Metadata dataframe containing patient information\n",
    "        train: Training subject indices\n",
    "        test: Testing subject indices\n",
    "    \n",
    "    Returns:\n",
    "        arrTrain: List of data indices for training samples\n",
    "        arrTest: List of data indices for testing samples\n",
    "    \"\"\"\n",
    "    arrTrain = []  # Store training data indices\n",
    "    arrTest = []   # Store testing data indices\n",
    "    \n",
    "    # Get all data indices for training subjects\n",
    "    for pID in arrName[train]:\n",
    "        idxofID = listData.index[listData['patientID']==pID].copy()  # Find all rows for this patient\n",
    "        arrTrain.extend(idxofID.to_list())  # Add patient's data indices to training set\n",
    "    \n",
    "    # Get all data indices for testing subjects\n",
    "    for pID in arrName[test]:\n",
    "        idxofID = listData.index[listData['patientID']==pID].copy()  # Find all rows for this patient\n",
    "        arrTest.extend(idxofID.to_list())  # Add patient's data indices to testing set\n",
    "    \n",
    "    return arrTrain, arrTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The order and count of listfileName and dataList must always match\n",
      "\n",
      "Num_listFromxlsx: 876 | Num_listFromFolder: 876\n",
      "Is same size: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(  patientID  dateVisit speed  numtrial    side  numStep\n",
       " 0      P002      31220     w         7  nonleg        1\n",
       " 1      P002      31220     w         7  nonleg        2\n",
       " 2      P002      31220     w         7   oaleg        1\n",
       " 3      P002      31220     w         8  nonleg        1\n",
       " 4      P002      31220     w         8   oaleg        1,\n",
       " ['N_F_P002_031220_w_0007_nonleg_imu_knee_angle_moment_R_1_Step.csv',\n",
       "  'N_F_P002_031220_w_0007_nonleg_imu_knee_angle_moment_R_2_Step.csv',\n",
       "  'N_F_P002_031220_w_0007_oaleg_imu_knee_angle_moment_R_1_Step.csv',\n",
       "  'N_F_P002_031220_w_0008_nonleg_imu_knee_angle_moment_R_1_Step.csv',\n",
       "  'N_F_P002_031220_w_0008_oaleg_imu_knee_angle_moment_R_1_Step.csv'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set required directories\n",
    "dataDir =     r'R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\INC_ByStep\\INC_ByZero\\Included_checked'\n",
    "normalizedDir = join(dataDir, r'NORM')\n",
    "\n",
    "#######################################################\n",
    "# Configuration window\n",
    "# This time using time-normalized data - all data lengths converted to 101 points\n",
    "TargetDir = normalizedDir\n",
    "#######################################################\n",
    "# Get file list\n",
    "# Get entire file list, select only needed extensions, and exclude everything except CSV files from the retrieved file list\n",
    "dataExt = r\".csv\"\n",
    "listFromFolder = natsorted([_ for _ in os.listdir(TargetDir) if _.endswith(dataExt)])\n",
    "# Load organized file list\n",
    "listfileName  = r'list_dataset_correction.xlsx'\n",
    "listFromxlsx = pd.read_excel(join(dataDir,listfileName))\n",
    "# Extract unique subjects\n",
    "arrName = listFromxlsx.patientID.unique()\n",
    "print(\"The order and count of listfileName and dataList must always match\")\n",
    "print(f\"\\nNum_listFromxlsx: {len(listFromxlsx)} | Num_listFromFolder: {len(listFromFolder)}\")\n",
    "print(f'Is same size: {len(listFromxlsx)==len(listFromFolder)}')\n",
    "listFromxlsx.head(), listFromFolder[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCC settings     BU Comuting Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '.\\\\NORM_CORRECTION'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#######################################################\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Get file list\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Get entire file list, select only needed extensions, and exclude everything except CSV files from the retrieved file list\u001b[39;00m\n\u001b[0;32m     11\u001b[0m dataExt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 12\u001b[0m listFromFolder \u001b[38;5;241m=\u001b[39m natsorted([_ \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTargetDir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m _\u001b[38;5;241m.\u001b[39mendswith(dataExt)])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load organized file list\u001b[39;00m\n\u001b[0;32m     14\u001b[0m listfileName  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist_dataset_correction.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '.\\\\NORM_CORRECTION'"
     ]
    }
   ],
   "source": [
    "# Set required directories\n",
    "dataDir =     r'.'\n",
    "normalizedDir = join(dataDir, r'NORM_CORRECTION')\n",
    "#######################################################\n",
    "# Configuration window\n",
    "# This time using time-normalized data - all data lengths converted to 101 points\n",
    "TargetDir = normalizedDir\n",
    "#######################################################\n",
    "# Get file list\n",
    "# Get entire file list, select only needed extensions, and exclude everything except CSV files from the retrieved file list\n",
    "dataExt = r\".csv\"\n",
    "listFromFolder = natsorted([_ for _ in os.listdir(TargetDir) if _.endswith(dataExt)])\n",
    "# Load organized file list\n",
    "listfileName  = r'list_dataset_correction.xlsx'\n",
    "listFromxlsx = pd.read_excel(join(dataDir,listfileName))\n",
    "# Extract unique subjects\n",
    "arrName = listFromxlsx.patientID.unique()\n",
    "print(\"The order and count of listfileName and dataList must always match\")\n",
    "print(f\"\\nNum_listFromxlsx: {len(listFromxlsx)} | Num_listFromFolder: {len(listFromFolder)}\")\n",
    "print(f'Is same size: {len(listFromxlsx)==len(listFromFolder)}')\n",
    "listFromxlsx.head(), listFromFolder[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset\n",
    "- Data split must be performed first before anything else!\n",
    "- Rather than splitting the dataset into train|valid|test (80|10|10), let's do 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required functions and class configuration\n",
    "def make_dir(file_path):\n",
    "    \"\"\"\n",
    "    Creates a directory if it doesn't already exist.\n",
    "    Safe directory creation that prevents overwriting existing folders.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "        \n",
    "# Used for scaling input data with (N, M, P) shape!\n",
    "# The function below is applied column-wise to the entire dataset!\n",
    "# Super convenient..\n",
    "class MinMaxScaler3D(MinMaxScaler):\n",
    "    \"\"\"\n",
    "    Custom MinMaxScaler for 3D data arrays.\n",
    "    Reshapes 3D data to 2D for scaling, then reshapes back to original dimensions.\n",
    "    Scales each feature independently while preserving temporal structure.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the scaler to 3D data by reshaping to 2D, fitting, then preserving parameters.\n",
    "        \"\"\"\n",
    "        x = np.reshape(X, newshape=(X.shape[0]*X.shape[1], X.shape[2]))  # Flatten first two dimensions\n",
    "        super().fit(x, y=y)  # Fit standard MinMaxScaler to reshaped data\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform 3D data using fitted scaler parameters.\n",
    "        Maintains original 3D shape after scaling.\n",
    "        \"\"\"\n",
    "        x = np.reshape(X, newshape=(X.shape[0]*X.shape[1], X.shape[2]))  # Reshape to 2D\n",
    "        return np.reshape(super().transform(x), newshape=X.shape)  # Scale and reshape back to 3D\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"\n",
    "        Reverse the scaling transformation to get back original scale values.\n",
    "        \"\"\"\n",
    "        x = np.reshape(X, newshape=(X.shape[0]*X.shape[1], X.shape[2]))  # Reshape to 2D\n",
    "        return np.reshape(super().inverse_transform(x), newshape=X.shape)  # Inverse scale and reshape back\n",
    "\n",
    "class StandardScaler3D(StandardScaler):\n",
    "    \"\"\"\n",
    "    Custom StandardScaler (Z-score normalization) for 3D data arrays.\n",
    "    Normalizes data to mean=0, std=1 while preserving 3D structure.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Calculate mean and standard deviation for each feature across all samples and time points.\n",
    "        \"\"\"\n",
    "        x = np.reshape(X, newshape=(X.shape[0]*X.shape[1], X.shape[2]))  # Flatten first two dimensions\n",
    "        super().fit(x, y=y)  # Fit standard StandardScaler\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply Z-score normalization: (x - mean) / std for each feature.\n",
    "        \"\"\"\n",
    "        x = np.reshape(X, newshape=(X.shape[0]*X.shape[1], X.shape[2]))  # Reshape to 2D\n",
    "        return np.reshape(super().transform(x), newshape=X.shape)  # Normalize and reshape back to 3D\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"\n",
    "        Convert normalized data back to original scale: (x * std) + mean.\n",
    "        \"\"\"\n",
    "        x = np.reshape(X, newshape=(X.shape[0]*X.shape[1], X.shape[2]))  # Reshape to 2D\n",
    "        return np.reshape(super().inverse_transform(x), newshape=X.shape)  # Denormalize and reshape back\n",
    "\n",
    "class RobustScaler3D(RobustScaler):\n",
    "    \"\"\"\n",
    "    Custom RobustScaler for 3D data arrays.\n",
    "    Uses median and interquartile range for scaling, robust to outliers.\n",
    "    Formula: (x - median) / IQR\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Calculate median and interquartile range for each feature.\n",
    "        More robust to outliers than StandardScaler.\n",
    "        \"\"\"\n",
    "        x = np.reshape(X, newshape=(X.shape[0]*X.shape[1], X.shape[2]))  # Flatten first two dimensions\n",
    "        super().fit(x, y=y)  # Fit standard RobustScaler\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply robust scaling using median and IQR instead of mean and std.\n",
    "        \"\"\"\n",
    "        x = np.reshape(X, newshape=(X.shape[0]*X.shape[1], X.shape[2]))  # Reshape to 2D\n",
    "        return np.reshape(super().transform(x), newshape=X.shape)  # Scale and reshape back to 3D\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"\n",
    "        Convert robust-scaled data back to original scale.\n",
    "        \"\"\"\n",
    "        x = np.reshape(X, newshape=(X.shape[0]*X.shape[1], X.shape[2]))  # Reshape to 2D\n",
    "        return np.reshape(super().inverse_transform(x), newshape=X.shape)  # Inverse scale and reshape back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For complete K-fold\n",
    "- Running this cell will save K pairs of (train set, test set, fitted scaler) for K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subject:44\n",
      "Total Data size:876\n",
      "+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+\n",
      "Num. of fold: 0\n",
      "\n",
      "['P002' 'P007' 'P017' 'P029' 'P050' 'P065' 'P104' 'P106' 'P115' 'P119'\n",
      " 'P134' 'P135' 'P136' 'P142' 'P147' 'P149' 'P155' 'P168' 'P169' 'P172'\n",
      " 'P196' 'P203' 'P222' 'P225' 'P226' 'P243' 'P245' 'P258' 'P263' 'P266'\n",
      " 'P270' 'P272' 'P273' 'P277' 'P290']\n",
      "Num for train:35\n",
      "\n",
      "['P061' 'P066' 'P069' 'P105' 'P121' 'P132' 'P205' 'P229' 'P297']\n",
      "Num for test:9\n",
      "\n",
      "Num total:44\n",
      "\n",
      "0_fold\n",
      "idx4train:722 | idx4test:154 | total:876/876\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73770b7939324911b216cc9304935484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_X = pd.concat([df_trainData_X, concated],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_Y_angle = pd.concat([df_trainData_Y_angle, angle],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:64: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_Y_moBWHT = pd.concat([df_trainData_Y_moBWHT, moBWHT],axis=0,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN data  :  722\n",
      "Final shape: (722, 4242, 1), (722, 303, 1), (722, 303, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f50e6534d14e97ba28eae1fe8aca41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:150: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_X = pd.concat([df_testData_X, concated],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:157: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_Y_angle = pd.concat([df_testData_Y_angle, angle],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:162: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_Y_moBWHT = pd.concat([df_testData_Y_moBWHT, moBWHT],axis=0,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST data  :  154\n",
      "Final shape: (154, 4242, 1), (154, 303, 1), (154, 303, 1)\n",
      "+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+\n",
      "Num. of fold: 1\n",
      "\n",
      "['P002' 'P007' 'P017' 'P029' 'P061' 'P065' 'P066' 'P069' 'P104' 'P105'\n",
      " 'P106' 'P115' 'P119' 'P121' 'P132' 'P134' 'P135' 'P142' 'P147' 'P149'\n",
      " 'P155' 'P168' 'P169' 'P172' 'P196' 'P205' 'P222' 'P225' 'P229' 'P245'\n",
      " 'P258' 'P263' 'P272' 'P290' 'P297']\n",
      "Num for train:35\n",
      "\n",
      "['P050' 'P136' 'P203' 'P226' 'P243' 'P266' 'P270' 'P273' 'P277']\n",
      "Num for test:9\n",
      "\n",
      "Num total:44\n",
      "\n",
      "1_fold\n",
      "idx4train:667 | idx4test:209 | total:876/876\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dde7bd4309b428b9bb7c1e794a755b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_X = pd.concat([df_trainData_X, concated],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_Y_angle = pd.concat([df_trainData_Y_angle, angle],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:64: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_Y_moBWHT = pd.concat([df_trainData_Y_moBWHT, moBWHT],axis=0,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN data  :  667\n",
      "Final shape: (667, 4242, 1), (667, 303, 1), (667, 303, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1ad407679749f88426e6772c886e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:150: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_X = pd.concat([df_testData_X, concated],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:157: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_Y_angle = pd.concat([df_testData_Y_angle, angle],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:162: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_Y_moBWHT = pd.concat([df_testData_Y_moBWHT, moBWHT],axis=0,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST data  :  209\n",
      "Final shape: (209, 4242, 1), (209, 303, 1), (209, 303, 1)\n",
      "+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+\n",
      "Num. of fold: 2\n",
      "\n",
      "['P002' 'P007' 'P029' 'P050' 'P061' 'P065' 'P066' 'P069' 'P104' 'P105'\n",
      " 'P115' 'P121' 'P132' 'P134' 'P136' 'P142' 'P147' 'P149' 'P155' 'P168'\n",
      " 'P172' 'P196' 'P203' 'P205' 'P226' 'P229' 'P243' 'P245' 'P258' 'P266'\n",
      " 'P270' 'P272' 'P273' 'P277' 'P297']\n",
      "Num for train:35\n",
      "\n",
      "['P017' 'P106' 'P119' 'P135' 'P169' 'P222' 'P225' 'P263' 'P290']\n",
      "Num for test:9\n",
      "\n",
      "Num total:44\n",
      "\n",
      "2_fold\n",
      "idx4train:722 | idx4test:154 | total:876/876\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077c788c7e094b1d91bb3cb3c19c2176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_X = pd.concat([df_trainData_X, concated],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_Y_angle = pd.concat([df_trainData_Y_angle, angle],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:64: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_Y_moBWHT = pd.concat([df_trainData_Y_moBWHT, moBWHT],axis=0,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN data  :  722\n",
      "Final shape: (722, 4242, 1), (722, 303, 1), (722, 303, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c2682a0c994ce399663b1cda6115ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:150: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_X = pd.concat([df_testData_X, concated],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:157: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_Y_angle = pd.concat([df_testData_Y_angle, angle],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:162: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_Y_moBWHT = pd.concat([df_testData_Y_moBWHT, moBWHT],axis=0,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST data  :  154\n",
      "Final shape: (154, 4242, 1), (154, 303, 1), (154, 303, 1)\n",
      "+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+\n",
      "Num. of fold: 3\n",
      "\n",
      "['P002' 'P007' 'P017' 'P050' 'P061' 'P066' 'P069' 'P105' 'P106' 'P115'\n",
      " 'P119' 'P121' 'P132' 'P134' 'P135' 'P136' 'P168' 'P169' 'P172' 'P203'\n",
      " 'P205' 'P222' 'P225' 'P226' 'P229' 'P243' 'P245' 'P258' 'P263' 'P266'\n",
      " 'P270' 'P273' 'P277' 'P290' 'P297']\n",
      "Num for train:35\n",
      "\n",
      "['P029' 'P065' 'P104' 'P142' 'P147' 'P149' 'P155' 'P196' 'P272']\n",
      "Num for test:9\n",
      "\n",
      "Num total:44\n",
      "\n",
      "3_fold\n",
      "idx4train:670 | idx4test:206 | total:876/876\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e036b8a70ff4df5915f0d1eb18f966d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/670 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_X = pd.concat([df_trainData_X, concated],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_Y_angle = pd.concat([df_trainData_Y_angle, angle],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:64: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_Y_moBWHT = pd.concat([df_trainData_Y_moBWHT, moBWHT],axis=0,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN data  :  670\n",
      "Final shape: (670, 4242, 1), (670, 303, 1), (670, 303, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557df576d4dd446a9938df1ce05b276a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:150: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_X = pd.concat([df_testData_X, concated],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:157: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_Y_angle = pd.concat([df_testData_Y_angle, angle],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:162: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_Y_moBWHT = pd.concat([df_testData_Y_moBWHT, moBWHT],axis=0,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST data  :  206\n",
      "Final shape: (206, 4242, 1), (206, 303, 1), (206, 303, 1)\n",
      "+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+\n",
      "Num. of fold: 4\n",
      "\n",
      "['P017' 'P029' 'P050' 'P061' 'P065' 'P066' 'P069' 'P104' 'P105' 'P106'\n",
      " 'P119' 'P121' 'P132' 'P135' 'P136' 'P142' 'P147' 'P149' 'P155' 'P169'\n",
      " 'P196' 'P203' 'P205' 'P222' 'P225' 'P226' 'P229' 'P243' 'P263' 'P266'\n",
      " 'P270' 'P272' 'P273' 'P277' 'P290' 'P297']\n",
      "Num for train:36\n",
      "\n",
      "['P002' 'P007' 'P115' 'P134' 'P168' 'P172' 'P245' 'P258']\n",
      "Num for test:8\n",
      "\n",
      "Num total:44\n",
      "\n",
      "4_fold\n",
      "idx4train:723 | idx4test:153 | total:876/876\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bff44782cee4457a4c33abe1cb30117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/723 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_X = pd.concat([df_trainData_X, concated],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_Y_angle = pd.concat([df_trainData_Y_angle, angle],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:64: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_Y_moBWHT = pd.concat([df_trainData_Y_moBWHT, moBWHT],axis=0,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN data  :  723\n",
      "Final shape: (723, 4242, 1), (723, 303, 1), (723, 303, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e8522fd6734cd899b77e3a14ee94a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:150: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_X = pd.concat([df_testData_X, concated],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:157: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_Y_angle = pd.concat([df_testData_Y_angle, angle],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\3609942896.py:162: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_Y_moBWHT = pd.concat([df_testData_Y_moBWHT, moBWHT],axis=0,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST data  :  153\n",
      "Final shape: (153, 4242, 1), (153, 303, 1), (153, 303, 1)\n"
     ]
    }
   ],
   "source": [
    "# Declare K-FOLD cross-validation\n",
    "kfold = KFold(n_splits=5, random_state=seed_rand, shuffle=True)\n",
    "# Always put the subject list in kfold.split(here)\n",
    "countfold = -1\n",
    "print(f\"Total subject:{len(arrName)}\")\n",
    "print(f\"Total Data size:{len(listFromxlsx)}\")\n",
    "for train,test in kfold.split(arrName):\n",
    "    # Fold numbering for current iteration\n",
    "    countfold = countfold + 1\n",
    "    # Create dataset\n",
    "    # Subject list used for this training session\n",
    "    print(\"+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+\")\n",
    "    print(f\"Num. of fold: {countfold}\\n\")\n",
    "    print(f'{arrName[train]}\\nNum for train:{len(arrName[train])}\\n')\n",
    "    print(f'{arrName[test]}\\nNum for test:{len(arrName[test])}\\n')\n",
    "    print(f'Num total:{len(arrName[train])+len(arrName[test])}')\n",
    "\n",
    "    idx4train,idx4test = kfold2subfold(arrName,listFromxlsx,train,test)\n",
    "    print(f\"\\n{countfold}_fold\\nidx4train:{len(idx4train)} | idx4test:{len(idx4test)} | total:{len(idx4train)+len(idx4test)}/{len(listFromxlsx)}\")\n",
    "    # Create complete list at once and use as needed\n",
    "    # Store all data and make it extractable by subject\n",
    "    columnsWOMAG = makeColumnsWOMAG()\n",
    "    X_columns = [str(i) for i in range(0,42)]\n",
    "    df_trainData_X = pd.DataFrame(columns=X_columns)\n",
    "\n",
    "    Y_columns = [str(i) for i in range(0,3)]\n",
    "    df_trainData_Y_angle = pd.DataFrame(columns=Y_columns) # angle has 3 axes\n",
    "    df_trainData_Y_moBWHT = pd.DataFrame(columns=Y_columns) # moment also has 3 axes\n",
    "    \n",
    "    # Store only data corresponding to training set!\n",
    "    # Valid and test sets can be created the same way!\n",
    "    for idx, datum in enumerate(tqdm([listFromFolder[i] for i in idx4train])):\n",
    "        df = pd.read_csv(join(TargetDir,datum))\n",
    "        # First exclude all MAG data from dataset\n",
    "        dfWOMAG = df.loc[:,columnsWOMAG]\n",
    "        # If measured moment leg is nonleg, keep file as is\n",
    "        if listFromxlsx.loc[idx,'side'] == \"oaleg\":\n",
    "            targetLegArr = dfWOMAG.loc[:,'oa_shank_ACC_X':'oa_thigh_GYRO_Z']\n",
    "            nonTargetLegArr = dfWOMAG.loc[:,'non_shank_ACC_X':'non_thigh_GYRO_Z'] # exclude mag\n",
    "            otherArr = dfWOMAG.loc[:,'trunk_ACC_X':'trunk_GYRO_Z'] \n",
    "        else:\n",
    "            targetLegArr = dfWOMAG.loc[:,'non_shank_ACC_X':'non_thigh_GYRO_Z'] # exclude mag\n",
    "            nonTargetLegArr = dfWOMAG.loc[:,'oa_shank_ACC_X':'oa_thigh_GYRO_Z'] # exclude mag\n",
    "            otherArr = dfWOMAG.loc[:,'trunk_ACC_X':'trunk_GYRO_Z']\n",
    "        # Always create data in the same order\n",
    "        concated = pd.concat([targetLegArr, nonTargetLegArr,otherArr],axis=1)\n",
    "\n",
    "        # Rename columns\n",
    "        # Need to stack data..\n",
    "        concated.columns = X_columns\n",
    "        # Accumulate input data\n",
    "        df_trainData_X = pd.concat([df_trainData_X, concated],axis=0,ignore_index=True)\n",
    "        ##############################################################\n",
    "        # Create output data\n",
    "        # kinematic(Angle)\n",
    "        angle = dfWOMAG.loc[:,'ANGLE_X':'ANGLE_Z']\n",
    "        angle.columns = Y_columns\n",
    "        # Accumulate output data\n",
    "        df_trainData_Y_angle = pd.concat([df_trainData_Y_angle, angle],axis=0,ignore_index=True)\n",
    "        # kinetic(moment)\n",
    "        moBWHT = dfWOMAG.loc[:,'MOBWHT_X':'MOBWHT_Z']\n",
    "        moBWHT.columns = Y_columns\n",
    "        # Accumulate output data\n",
    "        df_trainData_Y_moBWHT = pd.concat([df_trainData_Y_moBWHT, moBWHT],axis=0,ignore_index=True)\n",
    "    # Apply Scaler\n",
    "    # Currently only applying MinMaxScaler\n",
    "    scaler4X = MinMaxScaler() # Min-Max normalization\n",
    "    scaler4Y_angle = MinMaxScaler() # Min-Max normalization\n",
    "    scaler4Y_moBWHT  = MinMaxScaler() # Min-Max normalization\n",
    "\n",
    "    scaler4X.fit(df_trainData_X) # ONLY FOR TRAIN DATA!!!!ONLY FOR TRAIN DATA!!!!ONLY FOR TRAIN DATA!!!!\n",
    "    scaler4Y_angle.fit(df_trainData_Y_angle) # ONLY FOR TRAIN DATA!!!!ONLY FOR TRAIN DATA!!!!ONLY FOR TRAIN DATA!!!!\n",
    "    scaler4Y_moBWHT.fit(df_trainData_Y_moBWHT) # ONLY FOR TRAIN DATA!!!!ONLY FOR TRAIN DATA!!!!ONLY FOR TRAIN DATA!!!!\n",
    "\n",
    "    scaled_X_train = scaler4X.transform(df_trainData_X)\n",
    "    scaled_Y_angle_train = scaler4Y_angle.transform(df_trainData_Y_angle)\n",
    "    scaled_Y_moBWHT_train = scaler4Y_moBWHT.transform(df_trainData_Y_moBWHT)\n",
    "\n",
    "    # Scaler save location\n",
    "    scalerDir = join(dataDir, r'SAVE_dataSet',nameDataset)\n",
    "    make_dir(scalerDir)\n",
    "    # Save scaler\n",
    "    dump(scaler4X, open(join(scalerDir,f\"{countfold}_fold_scaler4X.pkl\"), 'wb'))\n",
    "    dump(scaler4Y_angle, open(join(scalerDir,f'{countfold}_fold_scaler4Y_angle.pkl'), 'wb'))\n",
    "    dump(scaler4Y_moBWHT, open(join(scalerDir,f'{countfold}_fold_scaler4Y_moBWHT.pkl'), 'wb'))\n",
    "\n",
    "    # Now that scaling is done properly, let's restore original structure!\n",
    "    # Desired shape format \n",
    "    # (N, 4242, 1), (N, 303, 1), (N, 303, 1)   N is number of data samples\n",
    "    X_train = []\n",
    "    Y_angle_train = []\n",
    "    Y_moBWHT_train = []\n",
    "    for i in range(0,len(idx4train)):\n",
    "        chopped_X_train = scaled_X_train[i*101:101+i*101,:]\n",
    "        X_train.append(chopped_X_train.flatten('F').reshape(-1,1))\n",
    "        \n",
    "        chopped_Y_angle_train= scaled_Y_angle_train[i*101:101+i*101,:]\n",
    "        Y_angle_train.append(chopped_Y_angle_train.flatten('F').reshape(-1,1))\n",
    "\n",
    "        chopped_Y_moBWHT_train= scaled_Y_moBWHT_train[i*101:101+i*101,:]\n",
    "        Y_moBWHT_train.append(chopped_Y_moBWHT_train.flatten('F').reshape(-1,1))\n",
    "\n",
    "    final_X_train = np.array(X_train)\n",
    "    final_Y_angle_train = np.array(Y_angle_train)\n",
    "    final_Y_moBWHT_train = np.array(Y_moBWHT_train)\n",
    "\n",
    "    # Check the shape of created data! \n",
    "    print(f'TRAIN data  :  {len(idx4train)}')\n",
    "    print(f'Final shape: {final_X_train.shape}, {final_Y_angle_train.shape}, {final_Y_moBWHT_train.shape}')\n",
    "\n",
    "    # Data save location\n",
    "    setDir = join(dataDir, r'SAVE_dataSet',nameDataset)\n",
    "    make_dir(setDir)\n",
    "    # Save data\n",
    "    savez_compressed(join(setDir,f\"{countfold}_fold_final_train.npz\"), final_X_train=final_X_train,final_Y_angle_train=final_Y_angle_train,final_Y_moBWHT_train=final_Y_moBWHT_train)\n",
    "    \n",
    "    #############################################################################################################################################\n",
    "    # For test set\n",
    "    # Create complete list at once and use as needed\n",
    "    # Store all data and make it extractable by subject\n",
    "    columnsWOMAG = makeColumnsWOMAG()\n",
    "    X_columns = [str(i) for i in range(0,42)]\n",
    "    df_testData_X = pd.DataFrame(columns=X_columns)\n",
    "\n",
    "    Y_columns = [str(i) for i in range(0,3)]\n",
    "    df_testData_Y_angle = pd.DataFrame(columns=Y_columns) # angle has 3 axes\n",
    "    df_testData_Y_moBWHT = pd.DataFrame(columns=Y_columns) # moment also has 3 axes\n",
    "    # Store only data corresponding to test set!\n",
    "    # Valid and test sets can be created the same way!\n",
    "    for idx, datum in enumerate(tqdm([listFromFolder[i] for i in idx4test])):\n",
    "        df = pd.read_csv(join(TargetDir,datum))\n",
    "        # First exclude all MAG data from dataset\n",
    "        dfWOMAG = df.loc[:,columnsWOMAG]\n",
    "        # If measured moment leg is nonleg, keep file as is\n",
    "        if listFromxlsx.loc[idx,'side'] == \"oaleg\":\n",
    "            targetLegArr = dfWOMAG.loc[:,'oa_shank_ACC_X':'oa_thigh_GYRO_Z']\n",
    "            nonTargetLegArr = dfWOMAG.loc[:,'non_shank_ACC_X':'non_thigh_GYRO_Z'] # exclude mag\n",
    "            otherArr = dfWOMAG.loc[:,'trunk_ACC_X':'trunk_GYRO_Z'] \n",
    "        else:\n",
    "            targetLegArr = dfWOMAG.loc[:,'non_shank_ACC_X':'non_thigh_GYRO_Z'] # exclude mag\n",
    "            nonTargetLegArr = dfWOMAG.loc[:,'oa_shank_ACC_X':'oa_thigh_GYRO_Z'] # exclude mag\n",
    "            otherArr = dfWOMAG.loc[:,'trunk_ACC_X':'trunk_GYRO_Z']\n",
    "        # Always create data in the same order\n",
    "        concated = pd.concat([targetLegArr, nonTargetLegArr,otherArr],axis=1)\n",
    "\n",
    "        # Rename columns\n",
    "        # Need to stack data..\n",
    "        concated.columns = X_columns\n",
    "        # Accumulate input data\n",
    "        df_testData_X = pd.concat([df_testData_X, concated],axis=0,ignore_index=True)\n",
    "        ##############################################################\n",
    "        # Create output data\n",
    "        # kinematic(Angle)\n",
    "        angle = dfWOMAG.loc[:,'ANGLE_X':'ANGLE_Z']\n",
    "        angle.columns = Y_columns\n",
    "        # Accumulate output data\n",
    "        df_testData_Y_angle = pd.concat([df_testData_Y_angle, angle],axis=0,ignore_index=True)\n",
    "        # kinetic(moment)\n",
    "        moBWHT = dfWOMAG.loc[:,'MOBWHT_X':'MOBWHT_Z']\n",
    "        moBWHT.columns = Y_columns\n",
    "        # Accumulate output data\n",
    "        df_testData_Y_moBWHT = pd.concat([df_testData_Y_moBWHT, moBWHT],axis=0,ignore_index=True)\n",
    "    \n",
    "    # Apply Scaler\n",
    "    scaled_X_test = scaler4X.transform(df_testData_X)\n",
    "    scaled_Y_angle_test = scaler4Y_angle.transform(df_testData_Y_angle)\n",
    "    scaled_Y_moBWHT_test = scaler4Y_moBWHT.transform(df_testData_Y_moBWHT)\n",
    "    \n",
    "    # Now that scaling is done properly, let's restore original structure!\n",
    "    # Desired shape format \n",
    "    # (N, 4242, 1), (N, 303, 1), (N, 303, 1)   N is number of data samples\n",
    "    X_test = []\n",
    "    Y_angle_test = []\n",
    "    Y_moBWHT_test = []\n",
    "    for i in range(0,len(idx4test)):\n",
    "        chopped_X_test = scaled_X_test[i*101:101+i*101,:]\n",
    "        X_test.append(chopped_X_test.flatten('F').reshape(-1,1))\n",
    "        \n",
    "        chopped_Y_angle_test= scaled_Y_angle_test[i*101:101+i*101,:]\n",
    "        Y_angle_test.append(chopped_Y_angle_test.flatten('F').reshape(-1,1))\n",
    "\n",
    "        chopped_Y_moBWHT_test= scaled_Y_moBWHT_test[i*101:101+i*101,:]\n",
    "        Y_moBWHT_test.append(chopped_Y_moBWHT_test.flatten('F').reshape(-1,1))\n",
    "\n",
    "    final_X_test = np.array(X_test)\n",
    "    final_Y_angle_test = np.array(Y_angle_test)\n",
    "    final_Y_moBWHT_test = np.array(Y_moBWHT_test)\n",
    "\n",
    "    # Check the shape of created data! \n",
    "    print(f'TEST data  :  {len(idx4test)}')\n",
    "    print(f'Final shape: {final_X_test.shape}, {final_Y_angle_test.shape}, {final_Y_moBWHT_test.shape}')\n",
    "\n",
    "    # Data save location\n",
    "    # Same as train data\n",
    "    # Save data\n",
    "    savez_compressed(join(setDir,f\"{countfold}_fold_final_test.npz\"), final_X_test=final_X_test,final_Y_angle_test=final_Y_angle_test,final_Y_moBWHT_test=final_Y_moBWHT_test)\n",
    "    # Delete later or make it into a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For individual K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mz:\\PROJECTS\\iwalqq\\Data\\V3D\\Output\\IMU Deep Learning\\IMUforKnee\\preperation\\4_DataSet_CAN_MYWAY.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/PROJECTS/iwalqq/Data/V3D/Output/IMU%20Deep%20Learning/IMUforKnee/preperation/4_DataSet_CAN_MYWAY.ipynb#ch0000012?line=0'>1</a>\u001b[0m \u001b[39m# KFOLD \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/z%3A/PROJECTS/iwalqq/Data/V3D/Output/IMU%20Deep%20Learning/IMUforKnee/preperation/4_DataSet_CAN_MYWAY.ipynb#ch0000012?line=1'>2</a>\u001b[0m kfold \u001b[39m=\u001b[39m KFold(n_splits\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m41\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/PROJECTS/iwalqq/Data/V3D/Output/IMU%20Deep%20Learning/IMUforKnee/preperation/4_DataSet_CAN_MYWAY.ipynb#ch0000012?line=2'>3</a>\u001b[0m \u001b[39m#    kfold.split()  \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/PROJECTS/iwalqq/Data/V3D/Output/IMU%20Deep%20Learning/IMUforKnee/preperation/4_DataSet_CAN_MYWAY.ipynb#ch0000012?line=3'>4</a>\u001b[0m countfold \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KFold' is not defined"
     ]
    }
   ],
   "source": [
    "# Declare K-FOLD cross-validation\n",
    "kfold = KFold(n_splits=5, random_state=41, shuffle=True)\n",
    "# Always put the subject list in kfold.split(here)\n",
    "countfold = -1\n",
    "for train,test in kfold.split(arrName):\n",
    "    # Numbering for current fold\n",
    "    countfold = countfold + 1\n",
    "    # Create dataset\n",
    "    # Subject list used for this training session\n",
    "    print(f\"Num. of fold: {countfold}\")\n",
    "    print(f'{arrName[train]}\\nNum for train:{len(arrName[train])}')\n",
    "    print(f'{arrName[test]}\\nNum for test:{len(arrName[test])}')\n",
    "    print(f'Num total:{len(arrName[train])+len(arrName[test])}')\n",
    "    idx4train,idx4test = kfold2subfold(arrName,listFromxlsx,train,test)\n",
    "    \n",
    "    break\n",
    "    # Delete later or turn into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(722, 155)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx4train), len(idx4test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b8906e303e4e6b80ac257c7b0c9e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/723 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\209504539.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_X = pd.concat([df_trainData_X, concated],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\209504539.py:40: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_Y_angle = pd.concat([df_trainData_Y_angle, angle],axis=0,ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\209504539.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_trainData_Y_moBWHT = pd.concat([df_trainData_Y_moBWHT, moBWHT],axis=0,ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Create complete list at once and set up for use as needed\n",
    "# Store all data and make it extractable by subject\n",
    "columnsWOMAG = makeColumnsWOMAG()\n",
    "X_columns = [str(i) for i in range(0,42)]\n",
    "df_trainData_X = pd.DataFrame(columns=X_columns)\n",
    "\n",
    "Y_columns = [str(i) for i in range(0,3)]\n",
    "df_trainData_Y_angle = pd.DataFrame(columns=Y_columns) # angle has 3 axes\n",
    "df_trainData_Y_moBWHT = pd.DataFrame(columns=Y_columns) # moment also has 3 axes\n",
    "\n",
    "# Store only data corresponding to training set!\n",
    "# Valid and test sets can be created the same way!\n",
    "for idx, datum in enumerate(tqdm([listFromFolder[i] for i in idx4train])):\n",
    "    df = pd.read_csv(join(TargetDir,datum))\n",
    "    # First exclude all MAG data from dataset\n",
    "    dfWOMAG = df.loc[:,columnsWOMAG]\n",
    "    # If measured moment leg is nonleg, keep file as is\n",
    "    if listFromxlsx.loc[idx,'side'] == \"oaleg\":\n",
    "        targetLegArr = dfWOMAG.loc[:,'oa_shank_ACC_X':'oa_thigh_GYRO_Z']\n",
    "        nonTargetLegArr = dfWOMAG.loc[:,'non_shank_ACC_X':'non_thigh_GYRO_Z'] # exclude mag\n",
    "        otherArr = dfWOMAG.loc[:,'trunk_ACC_X':'trunk_GYRO_Z'] \n",
    "    else:\n",
    "        targetLegArr = dfWOMAG.loc[:,'non_shank_ACC_X':'non_thigh_GYRO_Z'] # exclude mag\n",
    "        nonTargetLegArr = dfWOMAG.loc[:,'oa_shank_ACC_X':'oa_thigh_GYRO_Z'] # exclude mag\n",
    "        otherArr = dfWOMAG.loc[:,'trunk_ACC_X':'trunk_GYRO_Z']\n",
    "    # Always create data in the same order\n",
    "    concated = pd.concat([targetLegArr, nonTargetLegArr,otherArr],axis=1)\n",
    "\n",
    "    # Rename columns\n",
    "    # Need to stack data..\n",
    "    concated.columns = X_columns\n",
    "    # Accumulate input data\n",
    "    df_trainData_X = pd.concat([df_trainData_X, concated],axis=0,ignore_index=True)\n",
    "    ##############################################################\n",
    "    # Create output data\n",
    "    # kinematic(Angle)\n",
    "    angle = dfWOMAG.loc[:,'ANGLE_X':'ANGLE_Z']\n",
    "    angle.columns = Y_columns\n",
    "    # Accumulate output data\n",
    "    df_trainData_Y_angle = pd.concat([df_trainData_Y_angle, angle],axis=0,ignore_index=True)\n",
    "    # kinetic(moment)\n",
    "    moBWHT = dfWOMAG.loc[:,'MOBWHT_X':'MOBWHT_Z']\n",
    "    moBWHT.columns = Y_columns\n",
    "    # Accumulate output data\n",
    "    df_trainData_Y_moBWHT = pd.concat([df_trainData_Y_moBWHT, moBWHT],axis=0,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Scaler\n",
    "# Currently only applying MinMaxScaler (not MinMaxScaler3D)\n",
    "scaler4X = MinMaxScaler()  # Min-Max normalization (0-1 range)\n",
    "scaler4Y_angle = MinMaxScaler()  # Min-Max normalization (0-1 range) \n",
    "scaler4Y_moBWHT = MinMaxScaler()  # Min-Max normalization (0-1 range)\n",
    "\n",
    "# Fit scalers to training data only - critical for preventing data leakage\n",
    "scaler4X.fit(df_trainData_X)  # ONLY FOR TRAIN DATA!!!!ONLY FOR TRAIN DATA!!!!ONLY FOR TRAIN DATA!!!!\n",
    "scaler4Y_angle.fit(df_trainData_Y_angle)  # ONLY FOR TRAIN DATA!!!!ONLY FOR TRAIN DATA!!!!ONLY FOR TRAIN DATA!!!!\n",
    "scaler4Y_moBWHT.fit(df_trainData_Y_moBWHT)  # ONLY FOR TRAIN DATA!!!!ONLY FOR TRAIN DATA!!!!ONLY FOR TRAIN DATA!!!!\n",
    "\n",
    "# Apply scaling transformations using fitted parameters\n",
    "scaled_X_train = scaler4X.transform(df_trainData_X)\n",
    "scaled_Y_angle_train = scaler4Y_angle.transform(df_trainData_Y_angle)\n",
    "scaled_Y_moBWHT_train = scaler4Y_moBWHT.transform(df_trainData_Y_moBWHT)\n",
    "\n",
    "# Set scaler save location\n",
    "scalerDir = join(dataDir, r'SAVE_fittedScaler')\n",
    "make_dir(scalerDir)  # Create directory if it doesn't exist\n",
    "\n",
    "# Save fitted scalers for later use on test data and model deployment\n",
    "# Reference: https://wooono.tistory.com/360\n",
    "dump(scaler4X, open(join(scalerDir,f\"{countfold}_fold_scaler4X.pkl\"), 'wb'))  # Save input data scaler\n",
    "dump(scaler4Y_angle, open(join(scalerDir,f'{countfold}_fold_scaler4Y_angle.pkl'), 'wb'))  # Save angle output scaler\n",
    "dump(scaler4Y_moBWHT, open(join(scalerDir,f'{countfold}_fold_scaler4Y_moBWHT.pkl'), 'wb'))  # Save moment output scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've properly scaled the data, let's restore the original structure!\n",
    "# Desired shape format \n",
    "# (N, 4242, 1), (N, 303, 1), (N, 303, 1)   N is the number of data samples\n",
    "X_train = []\n",
    "Y_angle_train = []\n",
    "Y_moBWHT_train = []\n",
    "for i in range(0,len(idx4train)):\n",
    "    # Extract 101 time points for each sample from the scaled data\n",
    "    chopped_X_train = scaled_X_train[i*101:101+i*101,:]  # Get rows i*101 to (i+1)*101 for sample i\n",
    "    X_train.append(chopped_X_train.flatten('F').reshape(-1,1))  # Flatten in Fortran order and reshape to column vector\n",
    "    \n",
    "    chopped_Y_angle_train= scaled_Y_angle_train[i*101:101+i*101,:]  # Extract angle data for sample i\n",
    "    Y_angle_train.append(chopped_Y_angle_train.flatten('F').reshape(-1,1))  # Flatten and reshape to column vector\n",
    "    \n",
    "    chopped_Y_moBWHT_train= scaled_Y_moBWHT_train[i*101:101+i*101,:]  # Extract moment data for sample i\n",
    "    Y_moBWHT_train.append(chopped_Y_moBWHT_train.flatten('F').reshape(-1,1))  # Flatten and reshape to column vector\n",
    "\n",
    "# Convert lists to numpy arrays for final dataset format\n",
    "final_X_train = np.array(X_train)\n",
    "final_Y_angle_train = np.array(Y_angle_train)\n",
    "final_Y_moBWHT_train = np.array(Y_moBWHT_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN data  :  723\n",
      "Final shape: (723, 4242, 1), (723, 303, 1), (723, 303, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of created data! \n",
    "print(f'TRAIN data  :  {len(idx4train)}')\n",
    "print(f'Final shape: {final_X_train.shape}, {final_Y_angle_train.shape}, {final_Y_moBWHT_train.shape}')\n",
    "\n",
    "# Data save location\n",
    "setDir = join(dataDir, r'SAVE_dataSet')\n",
    "make_dir(setDir)  # Create directory if it doesn't exist\n",
    "\n",
    "# Save data\n",
    "# Reference: https://machinelearningmastery.com/how-to-save-a-numpy-array-to-file-for-machine-learning/\n",
    "savez_compressed(join(setDir,f\"{countfold}_fold_final_train.npz\"), \n",
    "                final_X_train=final_X_train,\n",
    "                final_Y_angle_train=final_Y_angle_train,\n",
    "                final_Y_moBWHT_train=final_Y_moBWHT_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded final shape: (723, 4242, 1), (723, 303, 1), (723, 303, 1)\n"
     ]
    }
   ],
   "source": [
    "# Verify data loading\n",
    "loaded_train = np.load(join(setDir,f\"{countfold}_fold_final_train.npz\"))\n",
    "print(f'Loaded final shape: {loaded_train[\"final_X_train\"].shape}, {loaded_train[\"final_Y_angle_train\"].shape}, {loaded_train[\"final_Y_moBWHT_train\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for presentation\n",
    "if True:\n",
    "    # Save scaled data for the first training sample\n",
    "    pd.DataFrame(final_X_train[0]).to_csv(\"CBD_IMU_1.csv\",index=False)\n",
    "    pd.DataFrame(final_Y_angle_train[0]).to_csv(\"CBD_angle_1.csv\",index=False)\n",
    "    pd.DataFrame(final_Y_moBWHT_train[0]).to_csv(\"CBD_moBWHT_1.csv\",index=False)\n",
    "    \n",
    "    # Restore to original data format! Confirmed this works well!\n",
    "    reshaped = final_X_train[0].reshape(-1,42, order='F')  # When restoring to original data!\n",
    "    result = scaler4X.inverse_transform(reshaped)\n",
    "    pd.DataFrame(result).to_csv(\"CBD_IMU_rescaled_1.csv\",index=False)\n",
    "    \n",
    "    reshaped = final_Y_angle_train[0].reshape(-1,3, order='F')  # When restoring to original data!\n",
    "    result = scaler4Y_angle.inverse_transform(reshaped)\n",
    "    pd.DataFrame(result).to_csv(\"CBD_angle_rescaled_1.csv\",index=False)\n",
    "    \n",
    "    reshaped = final_Y_moBWHT_train[0].reshape(-1,3, order='F')  # When restoring to original data!\n",
    "    result = scaler4Y_moBWHT.inverse_transform(reshaped)\n",
    "    pd.DataFrame(result).to_csv(\"CBD_moBWHT_rescaled_1.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5361cc64a1aa41f0a78e305ba8cfeccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\2689120845.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_X = pd.concat([df_testData_X, concated], axis=0, ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\2689120845.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_Y_angle = pd.concat([df_testData_Y_angle, angle], axis=0, ignore_index=True)\n",
      "C:\\Users\\asmith8\\AppData\\Local\\Temp\\ipykernel_33180\\2689120845.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_testData_Y_moBWHT = pd.concat([df_testData_Y_moBWHT, moBWHT], axis=0, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Set up data structures for test dataset creation\n",
    "# Define column structure without magnetometer data for consistency\n",
    "columnsWOMAG = makeColumnsWOMAG()\n",
    "X_columns = [str(i) for i in range(0,42)]  # 42 sensor features (ACC+GYRO only, no MAG)\n",
    "df_testData_X = pd.DataFrame(columns=X_columns)\n",
    "\n",
    "Y_columns = [str(i) for i in range(0,3)]  # 3-axis outputs (X, Y, Z)\n",
    "df_testData_Y_angle = pd.DataFrame(columns=Y_columns)  # Joint angle predictions\n",
    "df_testData_Y_moBWHT = pd.DataFrame(columns=Y_columns)  # Joint moment predictions\n",
    "\n",
    "# Process each test file to build consolidated test dataset\n",
    "# Use same preprocessing pipeline as training data to ensure consistency\n",
    "for idx, datum in enumerate(tqdm([listFromFolder[i] for i in idx4test])):\n",
    "    df = pd.read_csv(join(TargetDir,datum))\n",
    "    \n",
    "    # Remove magnetometer columns to focus on accelerometer and gyroscope data\n",
    "    dfWOMAG = df.loc[:,columnsWOMAG]\n",
    "    \n",
    "    # Standardize sensor arrangement regardless of which leg was measured\n",
    "    # Critical: Always put target leg first for consistent neural network input\n",
    "    if listFromxlsx.loc[idx,'side'] == \"oaleg\":\n",
    "        # If operated leg was measured, arrange as: operated leg -> non-operated leg -> trunk\n",
    "        targetLegArr = dfWOMAG.loc[:,'oa_shank_ACC_X':'oa_thigh_GYRO_Z']\n",
    "        nonTargetLegArr = dfWOMAG.loc[:,'non_shank_ACC_X':'non_thigh_GYRO_Z']\n",
    "        otherArr = dfWOMAG.loc[:,'trunk_ACC_X':'trunk_GYRO_Z'] \n",
    "    else:\n",
    "        # If non-operated leg was measured, arrange as: non-operated leg -> operated leg -> trunk\n",
    "        targetLegArr = dfWOMAG.loc[:,'non_shank_ACC_X':'non_thigh_GYRO_Z']\n",
    "        nonTargetLegArr = dfWOMAG.loc[:,'oa_shank_ACC_X':'oa_thigh_GYRO_Z']\n",
    "        otherArr = dfWOMAG.loc[:,'trunk_ACC_X':'trunk_GYRO_Z']\n",
    "    \n",
    "    # Concatenate sensor data in standardized order: target leg, non-target leg, trunk\n",
    "    concated = pd.concat([targetLegArr, nonTargetLegArr, otherArr], axis=1)\n",
    "\n",
    "    # Rename columns to generic numbers for easier processing downstream\n",
    "    concated.columns = X_columns\n",
    "    \n",
    "    # Append this sample's input features to the growing test dataset\n",
    "    df_testData_X = pd.concat([df_testData_X, concated], axis=0, ignore_index=True)\n",
    "    \n",
    "    ##############################################################\n",
    "    # Extract output targets that the model will learn to predict\n",
    "    \n",
    "    # Joint kinematics (angles) - how much the joint bends in each direction\n",
    "    angle = dfWOMAG.loc[:,'ANGLE_X':'ANGLE_Z']\n",
    "    angle.columns = Y_columns\n",
    "    df_testData_Y_angle = pd.concat([df_testData_Y_angle, angle], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Joint kinetics (moments) - forces/torques applied at the joint\n",
    "    moBWHT = dfWOMAG.loc[:,'MOBWHT_X':'MOBWHT_Z']\n",
    "    moBWHT.columns = Y_columns\n",
    "    df_testData_Y_moBWHT = pd.concat([df_testData_Y_moBWHT, moBWHT], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaling to test data using previously fitted scalers\n",
    "# Critical: Use scalers fitted ONLY on training data to prevent data leakage\n",
    "scaled_X_test = scaler4X.transform(df_testData_X)  # Scale test input features using training data statistics\n",
    "scaled_Y_angle_test = scaler4Y_angle.transform(df_testData_Y_angle)  # Scale test angle outputs using training data statistics\n",
    "scaled_Y_moBWHT_test = scaler4Y_moBWHT.transform(df_testData_Y_moBWHT)  # Scale test moment outputs using training data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've properly scaled the data, let's separate it back into individual samples!\n",
    "# Desired shape format \n",
    "# (N, 4242, 1), (N, 303, 1), (N, 303, 1)   N is the number of data samples\n",
    "X_test = []\n",
    "Y_angle_test = []\n",
    "Y_moBWHT_test = []\n",
    "for i in range(0,len(idx4test)):\n",
    "    # Extract 101 time points for each test sample from the scaled data\n",
    "    chopped_X_test = scaled_X_test[i*101:101+i*101,:]  # Get rows i*101 to (i+1)*101 for sample i\n",
    "    X_test.append(chopped_X_test.flatten('F').reshape(-1,1))  # Flatten in Fortran order and reshape to column vector\n",
    "    \n",
    "    chopped_Y_angle_test= scaled_Y_angle_test[i*101:101+i*101,:]  # Extract angle data for sample i\n",
    "    Y_angle_test.append(chopped_Y_angle_test.flatten('F').reshape(-1,1))  # Flatten and reshape to column vector\n",
    "    \n",
    "    chopped_Y_moBWHT_test= scaled_Y_moBWHT_test[i*101:101+i*101,:]  # Extract moment data for sample i\n",
    "    Y_moBWHT_test.append(chopped_Y_moBWHT_test.flatten('F').reshape(-1,1))  # Flatten and reshape to column vector\n",
    "\n",
    "# Convert lists to numpy arrays for final test dataset format\n",
    "final_X_test = np.array(X_test)\n",
    "final_Y_angle_test = np.array(Y_angle_test)\n",
    "final_Y_moBWHT_test = np.array(Y_moBWHT_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST data  :  153\n",
      "Final shape: (153, 4242, 1), (153, 303, 1), (153, 303, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of created data! \n",
    "print(f'TEST data  :  {len(idx4test)}')\n",
    "print(f'Final shape: {final_X_test.shape}, {final_Y_angle_test.shape}, {final_Y_moBWHT_test.shape}')\n",
    "\n",
    "# Data save location\n",
    "setDir = join(dataDir, r'SAVE_dataSet')\n",
    "make_dir(setDir)  # Create directory if it doesn't exist\n",
    "\n",
    "# Save data\n",
    "savez_compressed(join(setDir,f\"{countfold}_fold_final_test.npz\"), \n",
    "                final_X_test=final_X_test,\n",
    "                final_Y_angle_test=final_Y_angle_test,\n",
    "                final_Y_moBWHT_test=final_Y_moBWHT_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded final shape: (153, 4242, 1), (153, 303, 1), (153, 303, 1)\n"
     ]
    }
   ],
   "source": [
    "# Verify data loading\n",
    "loaded_test = np.load(join(setDir,f\"{countfold}_fold_final_test.npz\"))\n",
    "print(f'Loaded final shape: {loaded_test[\"final_X_test\"].shape}, {loaded_test[\"final_Y_angle_test\"].shape}, {loaded_test[\"final_Y_moBWHT_test\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What was newly learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4242, 1), (101, 42, 1))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to reshape data\n",
    "flat_concat.shape, concated.to_numpy().reshape(101,42,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3]), (4, 3))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 2D numpy array with 4 rows and 3 columns\n",
    "a = np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\n",
    "\n",
    "# Extract the first row (all columns) and check the overall array shape\n",
    "a[0,:], a.shape  # Returns: (array([1, 2, 3]), (4, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.08866167, -1.6116709 , -1.12578921, -0.69913567, -0.31552896,\n",
       "        0.01626456,  0.2934858 ,  0.49830202,  0.62481276,  0.67396727,\n",
       "        0.66056931,  0.60059968,  0.50080002,  0.36252644,  0.18663478,\n",
       "       -0.0244365 , -0.26707541, -0.52839363, -0.78795609, -1.01967686,\n",
       "       -1.20064522, -1.31313809, -1.34727698, -1.30148371, -1.18328261,\n",
       "       -1.01031294, -0.80652645, -0.59887084, -0.41240249, -0.26469963,\n",
       "       -0.1639805 , -0.11075129, -0.09801369, -0.11699963, -0.1552758 ,\n",
       "       -0.19970287, -0.2355143 , -0.24934117, -0.23495442, -0.19335586,\n",
       "       -0.13348058, -0.066768  , -0.00379263,  0.04791029,  0.08493437,\n",
       "        0.10705636,  0.1168679 ,  0.11778726,  0.1139358 ,  0.10917843,\n",
       "        0.10651575,  0.10806219,  0.11395871,  0.12298314,  0.13297936,\n",
       "        0.14167265,  0.14704517,  0.14762067,  0.14238325,  0.13102786,\n",
       "        0.11354904,  0.09108217,  0.06584669,  0.04045179,  0.01679698,\n",
       "       -0.00449873, -0.02328051, -0.03918528, -0.0509166 , -0.05659852,\n",
       "       -0.05403333, -0.04182458, -0.01860489,  0.01560236,  0.0597202 ,\n",
       "        0.11148928,  0.16782661,  0.22637405,  0.28447062,  0.34224201,\n",
       "        0.39966642,  0.45754159,  0.5165377 ,  0.57683125,  0.63907464,\n",
       "        0.7016639 ,  0.76295203,  0.81993773,  0.86906512,  0.90632289,\n",
       "        0.92722147,  0.92593681,  0.89690778,  0.83346197,  0.7289472 ,\n",
       "        0.58455905,  0.39661491,  0.18008911, -0.07270238, -0.32650802,\n",
       "       -0.60742325])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape must be done in order: (rows, columns, count) to be correct\n",
    "# concated consists of 101 rows and 42 columns\n",
    "concated.to_numpy().reshape(101,42,1)[:,0,0]  # Reads in the correct direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P002' 'P007' 'P017' 'P029' 'P061' 'P065' 'P069' 'P104' 'P105' 'P106'\n",
      " 'P115' 'P119' 'P121' 'P135' 'P147' 'P149' 'P155' 'P168' 'P169' 'P172'\n",
      " 'P196' 'P203' 'P222' 'P225' 'P229' 'P243' 'P245' 'P258' 'P263' 'P266'\n",
      " 'P270' 'P272' 'P273' 'P277' 'P290']\n",
      "['P050' 'P066' 'P132' 'P134' 'P136' 'P142' 'P205' 'P226' 'P297']\n",
      "35,9\n",
      "+++++++++++\n",
      "['P002' 'P007' 'P017' 'P029' 'P061' 'P065' 'P069' 'P104' 'P105' 'P106'\n",
      " 'P115' 'P119' 'P121' 'P135' 'P147' 'P169' 'P172' 'P203' 'P225' 'P229'\n",
      " 'P243' 'P245' 'P258' 'P266' 'P270' 'P272' 'P273' 'P290']\n",
      "['P149' 'P155' 'P168' 'P196' 'P222' 'P263' 'P277']\n",
      "28,7\n"
     ]
    }
   ],
   "source": [
    "# When you need to split into train/valid/test (two-level splitting)\n",
    "arrName = np.array(['P002', 'P007', 'P017', 'P029', 'P050', 'P061', 'P065', 'P066',\n",
    "       'P069', 'P104', 'P105', 'P106', 'P115', 'P119', 'P121', 'P132',\n",
    "       'P134', 'P135', 'P136', 'P142', 'P147', 'P149', 'P155', 'P168',\n",
    "       'P169', 'P172', 'P196', 'P203', 'P205', 'P222', 'P225', 'P226',\n",
    "       'P229', 'P243', 'P245', 'P258', 'P263', 'P266', 'P270', 'P272',\n",
    "       'P273', 'P277', 'P290', 'P297'])\n",
    "\n",
    "# Preliminary work for K-fold cross-validation\n",
    "# Enable folding by subject\n",
    "# First declare K-FOLD\n",
    "kfold = KFold(n_splits=5, random_state=4, shuffle=True)\n",
    "\n",
    "# Always put the subject list in kfold.split\n",
    "for train_whole,test in kfold.split(arrName):\n",
    "    # Create dataset\n",
    "    # Subject list used for this training session\n",
    "    print(arrName[train_whole])  # Subjects for training+validation combined\n",
    "    print(arrName[test])         # Subjects for final testing\n",
    "    print(f'{len(arrName[train_whole])},{len(arrName[test])}')\n",
    "    print(\"+++++++++++\")\n",
    "    break\n",
    "\n",
    "# Second split: divide training subjects into training and validation\n",
    "arrTWhole = arrName[train_whole]  # Take the training+validation subjects\n",
    "for train,valid in kfold.split(arrTWhole):\n",
    "    print(arrTWhole[train])  # Final training subjects\n",
    "    print(arrTWhole[valid])  # Validation subjects  \n",
    "    print(f'{len(arrTWhole[train])},{len(arrName[valid])}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/questions/50125844/how-to-standard-scale-a-3d-matrix\n",
    "# God bless this programmer..\n",
    "# Scaling procedure/workflow\n",
    "X_tmp = X_train.copy()  # Create a copy to avoid modifying original data\n",
    "\n",
    "# Declare/initialize the 3D scaler\n",
    "scaler = StandardScaler3D()\n",
    "\n",
    "# Fit the scaler to learn scaling parameters (mean and std)\n",
    "scaler.fit(X_tmp)\n",
    "\n",
    "# Apply scaling transformation using learned parameters\n",
    "scaled_X_tmp = scaler.transform(X_tmp)\n",
    "\n",
    "# Reverse scaling transformation to get back original values\n",
    "rescaled_X_tmp = scaler.inverse_transform(scaled_X_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "027f9cdfda7de9e7e2f38ae898d6096d85a1b774378998807bc05733ce5ad18d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
