{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from pickle import load\n",
    "import shutil\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import keras.backend as K\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "# Model 생성, compile\n",
    "modelType = 'Dense'\n",
    "def create_model():\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(4242,)),\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(6000, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(4000, activation='relu'),  \n",
    "    tf.keras.layers.Dense(303, activation='linear'),  \n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "learningRate = 0.0008 # 두번째 실험\n",
    "patience = 10\n",
    "myoptim=Nadam(learning_rate=learningRate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0)\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience = patience, mode='min') # 일단은 적당히 epoch 주고 돌리기\n",
    "\n",
    "# 원래의 범위로 값 평가하기\n",
    "# 원래 정확도 복구해서 tensorboard에 기록하기\n",
    "def rescaled_RMSE(y_true, y_pred):\n",
    "    y_true = tf.reshape(tf.squeeze(y_true), [-1,3])\n",
    "    y_pred = tf.reshape(tf.squeeze(y_pred), [-1,3])\n",
    "    y_true = (y_true - K.constant(load_scaler4Y_angle.min_)) / K.constant(load_scaler4Y_angle.scale_)\n",
    "    y_pred = (y_pred - K.constant(load_scaler4Y_angle.min_)) / K.constant(load_scaler4Y_angle.scale_)\n",
    "    # default is RMSE, squaredbool, default=True If True returns MSE value, if False returns RMSE value.\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n",
    "    rescaled_RMSE = K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    print(f\"\\nUsing {numFold} fold scaler\")\n",
    "    return rescaled_RMSE\n",
    "\n",
    "def X_Axis_RMSE(y_true, y_pred):\n",
    "    NumAxis = 0\n",
    "    y_true = tf.reshape(tf.squeeze(y_true), [-1,3])[NumAxis]\n",
    "    y_pred = tf.reshape(tf.squeeze(y_pred), [-1,3])[NumAxis]\n",
    "    print(y_true.shape)\n",
    "    y_true = (y_true - K.constant(load_scaler4Y_angle.min_[NumAxis])) / K.constant(load_scaler4Y_angle.scale_[NumAxis])\n",
    "    y_pred = (y_pred - K.constant(load_scaler4Y_angle.min_[NumAxis])) / K.constant(load_scaler4Y_angle.scale_[NumAxis])\n",
    "    # default is RMSE, squaredbool, default=True If True returns MSE value, if False returns RMSE value.\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n",
    "    X_Axis_RMSE = K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    print(f\"\\nUsing {numFold} fold scaler\")\n",
    "    return X_Axis_RMSE\n",
    "    \n",
    "def Y_Axis_RMSE(y_true, y_pred):\n",
    "    NumAxis = 1\n",
    "    y_true = tf.reshape(tf.squeeze(y_true), [-1,3])[NumAxis]\n",
    "    y_pred = tf.reshape(tf.squeeze(y_pred), [-1,3])[NumAxis]\n",
    "    print(y_true.shape)\n",
    "    y_true = (y_true - K.constant(load_scaler4Y_angle.min_[NumAxis])) / K.constant(load_scaler4Y_angle.scale_[NumAxis])\n",
    "    y_pred = (y_pred - K.constant(load_scaler4Y_angle.min_[NumAxis])) / K.constant(load_scaler4Y_angle.scale_[NumAxis])\n",
    "    # default is RMSE, squaredbool, default=True If True returns MSE value, if False returns RMSE value.\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n",
    "    Y_Axis_RMSE = K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    print(f\"\\nUsing {numFold} fold scaler\")\n",
    "    return Y_Axis_RMSE\n",
    "\n",
    "def Z_Axis_RMSE(y_true, y_pred):\n",
    "    NumAxis = 2\n",
    "    y_true = tf.reshape(tf.squeeze(y_true), [-1,3])[NumAxis]\n",
    "    y_pred = tf.reshape(tf.squeeze(y_pred), [-1,3])[NumAxis]\n",
    "    print(y_true.shape)\n",
    "    y_true = (y_true - K.constant(load_scaler4Y_angle.min_[NumAxis])) / K.constant(load_scaler4Y_angle.scale_[NumAxis])\n",
    "    y_pred = (y_pred - K.constant(load_scaler4Y_angle.min_[NumAxis])) / K.constant(load_scaler4Y_angle.scale_[NumAxis])\n",
    "    # default is RMSE, squaredbool, default=True If True returns MSE value, if False returns RMSE value.\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n",
    "    Z_Axis_RMSE = K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    print(f\"\\nUsing {numFold} fold scaler\")\n",
    "    return Z_Axis_RMSE\n",
    "\n",
    "\n",
    "\n",
    "# 학습 조건 저장하기\n",
    "config = dict()\n",
    "config[\"model\"] = modelType\n",
    "config['patience'] = patience\n",
    "\n",
    "# 데이터 셋 준비\n",
    "# 데이터 셋 준비\n",
    "dataSetDir = r'Z:\\PROJECTS\\iwalqq\\Data\\V3D\\Output\\IMU Deep Learning\\Data\\20220325_raw_byDeepak_csv\\INC_ByStep\\INC_ByZero\\Included_checked\\SAVE_dataSet'\n",
    "scalerDir  = r'Z:\\PROJECTS\\iwalqq\\Data\\V3D\\Output\\IMU Deep Learning\\Data\\20220325_raw_byDeepak_csv\\INC_ByStep\\INC_ByZero\\Included_checked\\SAVE_fittedScaler'\n",
    "\n",
    "# 준비된 K-fold data iterations\n",
    "time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "SaveModelDir = 'SavedModel'\n",
    "# shutil.rmtree('./logs', ignore_errors=True) # 삭제는 신중ㅎ히\n",
    "# tensorboard 동작시키는 법 : tensorboard --logdir logs/fit\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "for numFold in range(0,5): # 5-fold crossvalidation\n",
    "    # 각 fold 별로 별도로 표기하기\n",
    "    log_dir = \"logs/fit/\" + str(numFold) + '_fold_' + time\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    print(f\"Num of Fold: {numFold}\")\n",
    "    # 데이터 불러오기\n",
    "    # 모든 데이터는 scaled된 데이터임!\n",
    "    load_train = np.load(join(dataSetDir,f\"{numFold}_fold_final_train.npz\"))\n",
    "    load_test = np.load(join(dataSetDir,f\"{numFold}_fold_final_test.npz\"))\n",
    "    print(f'loaded Train shape: {load_train[\"final_X_train\"].shape}, {load_train[\"final_Y_angle_train\"].shape}, {load_train[\"final_Y_moBWHT_train\"].shape}')\n",
    "    print(f'loaded Test shape: {load_test[\"final_X_test\"].shape}, {load_test[\"final_Y_angle_test\"].shape}, {load_test[\"final_Y_moBWHT_test\"].shape}')\n",
    "    # sclaer 불러오기\n",
    "    # Here scaler is MinMaxScaler!\n",
    "    load_scaler4X = load(open(join(scalerDir,f\"{numFold}_fold_scaler4X.pkl\"), 'rb'))\n",
    "    load_scaler4Y_angle = load(open(join(scalerDir,f\"{numFold}_fold_scaler4Y_angle.pkl\"), 'rb'))\n",
    "    load_scaler4Y_moBWHT = load(open(join(scalerDir,f\"{numFold}_fold_scaler4Y_moBWHT.pkl\"), 'rb'))\n",
    "\n",
    "    # https://wandb.ai/sauravm/Optimizers/reports/How-to-Compare-Keras-Optimizers-in-Tensorflow--VmlldzoxNjU1OTA4\n",
    "    # Nadam을 선택한 이유\n",
    "    model = create_model()\n",
    "    model.compile(optimizer=myoptim,\n",
    "              loss='mean_absolute_error',\n",
    "              metrics=[rescaled_RMSE, X_Axis_RMSE, Y_Axis_RMSE, Z_Axis_RMSE])\n",
    "    # 차원 축소\n",
    "    X_train = np.squeeze(load_train[\"final_X_train\"], axis=2)\n",
    "    Y_angle_train = np.squeeze(load_train[\"final_Y_angle_train\"], axis=2)\n",
    "\n",
    "    X_test = np.squeeze(load_test[\"final_X_test\"], axis=2)\n",
    "    Y_angle_test = np.squeeze(load_test[\"final_Y_angle_test\"], axis=2)\n",
    "\n",
    "    # 요건 나중에... [early_stopping,]\n",
    "    history = model.fit(X_train, Y_angle_train, validation_data=(X_test,Y_angle_test), epochs=epochs, callbacks=[tensorboard_callback])\n",
    "\n",
    "    # 모델은 확실해지면 저장하기\n",
    "    # model.save(join(SaveModelDir,f\"{numFold}_fold_my_model_\"+time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 배운 것!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = (y_true - K.constant(load_scaler4Y_angle.min_)) / K.constant(load_scaler4Y_angle.scale_)\n",
    "y_pred = (y_pred - K.constant(load_scaler4Y_angle.min_)) / K.constant(load_scaler4Y_angle.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/55857212/scaling-back-data-in-customized-keras-training-loss-function\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[-1, 2], [-0.5, 6], [0, 10], [1, 18]])\n",
    "print('data:\\n',data)\n",
    "scaler = MinMaxScaler()\n",
    "data_trans = scaler.fit_transform(data)\n",
    "print('transform:\\n',data_trans)\n",
    "\n",
    "data_inverse = (data_trans - scaler.min_)/scaler.scale_\n",
    "print('inverse transform:\\n',data_inverse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(data_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.09092975]\n",
      " [0.12411916]\n",
      " [0.11097777]\n",
      " [0.90343535]\n",
      " [0.65863633]\n",
      " [0.5038903 ]\n",
      " [0.81531835]\n",
      " [0.11922765]\n",
      " [0.44267535]\n",
      " [0.12487292]\n",
      " [0.25096822]\n",
      " [0.01589823]\n",
      " [0.45424676]\n",
      " [0.9813316 ]\n",
      " [0.64427793]\n",
      " [0.13936126]\n",
      " [0.7115203 ]\n",
      " [0.03113258]\n",
      " [0.37205803]\n",
      " [0.5225203 ]\n",
      " [0.83543   ]\n",
      " [0.18276966]\n",
      " [0.5997149 ]\n",
      " [0.87808084]\n",
      " [0.55225885]\n",
      " [0.0841701 ]\n",
      " [0.543236  ]\n",
      " [0.43449676]\n",
      " [0.66344714]\n",
      " [0.4394734 ]\n",
      " [0.4461223 ]\n",
      " [0.75619256]\n",
      " [0.96090126]\n",
      " [0.82392764]\n",
      " [0.57704854]\n",
      " [0.7458937 ]\n",
      " [0.02869189]\n",
      " [0.9994967 ]\n",
      " [0.1574409 ]\n",
      " [0.5164174 ]\n",
      " [0.37056577]\n",
      " [0.26939178]\n",
      " [0.9467038 ]\n",
      " [0.00451362]\n",
      " [0.15430284]\n",
      " [0.64233613]\n",
      " [0.52080226]\n",
      " [0.05968118]\n",
      " [0.6821805 ]\n",
      " [0.1590873 ]], shape=(50, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.09092975 0.12411916 0.11097777 0.90343535 0.65863633]\n",
      " [0.5038903  0.81531835 0.11922765 0.44267535 0.12487292]\n",
      " [0.25096822 0.01589823 0.45424676 0.9813316  0.64427793]\n",
      " [0.13936126 0.7115203  0.03113258 0.37205803 0.5225203 ]\n",
      " [0.83543    0.18276966 0.5997149  0.87808084 0.55225885]\n",
      " [0.0841701  0.543236   0.43449676 0.66344714 0.4394734 ]\n",
      " [0.4461223  0.75619256 0.96090126 0.82392764 0.57704854]\n",
      " [0.7458937  0.02869189 0.9994967  0.1574409  0.5164174 ]\n",
      " [0.37056577 0.26939178 0.9467038  0.00451362 0.15430284]\n",
      " [0.64233613 0.52080226 0.05968118 0.6821805  0.1590873 ]], shape=(10, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "logits = tf.random.uniform(([50,1]))\n",
    "print(logits)\n",
    "print(tf.reshape(logits,shape=[10,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6afbe383263539ad7050e063937c55ebe70595122907f9c14e92dfb99f36908c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('buIMU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
