{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_Checker\n",
    "### Purpose\n",
    "- Save data step by step\n",
    "- Identify data to exclude\n",
    "- Automatically exclude data that should be removed\n",
    "- Data to exclude is located in \"20220325_raw_byDeepak_csv\" folder\n",
    "### Data needs to be modified based on which foot was stepped on\n",
    "### Output Example\n",
    "- Data_Checked\n",
    "    - list of data.xlsx\n",
    "    - [folder] include\n",
    "    - [folder] exclude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (3.9.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from matplotlib) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asmith8\\.conda\\envs\\imu_env39\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from natsort import natsorted\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "from tqdm.notebook import tqdm\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import mpl_toolkits.axisartist as AA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare necessary functions\n",
    "def ensure_dir(file_path):\n",
    "    \"\"\"Creates a directory if it doesn't already exist\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclude_insufficient_length and step-wise saving\n",
    "# - Exclude data that doesn't meet minimum length requirements\n",
    "# - Classify and save data into 1-step and 2-step categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, divide data by steps\n",
    "# Load complete data list\n",
    "# Specify target directories\n",
    "dataDir =       r\"R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\"\n",
    "rawDir =        r\"R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\RAW\"\n",
    "includeDir =    r\"R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\INC_ByStep\\RAW\"\n",
    "excludeDir =    r\"R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\EXC\"\n",
    "dataExt = r\".csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f634c9167a7a4e70be25244247a8ee47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Create directories if they don't exist\n",
    "ensure_dir(includeDir)\n",
    "ensure_dir(excludeDir)\n",
    "\n",
    "# Get list of all files in raw directory, filter for CSV files only\n",
    "dataList = natsorted([_ for _ in os.listdir(rawDir) if _.endswith(dataExt)])\n",
    "excluded_len = []  # Track number of steps extracted per file (0 = excluded)\n",
    "\n",
    "# Process each file to check data quality and extract individual steps\n",
    "# Quality checks:\n",
    "# 1) Does the file have complete data? Must have at least 1 step of IMU + mechanics data (minimum 75 columns)\n",
    "# 2) Does the data contain repeated zeros indicating corrupted/incomplete recordings?\n",
    "\n",
    "Num_column_start = 2  # Skip first 2 empty columns before data begins\n",
    "\n",
    "for datum in tqdm(dataList):\n",
    "    # Read individual CSV file\n",
    "    read_file = pd.read_csv(os.path.join(rawDir, datum))\n",
    "    \n",
    "    # Check if file has valid data structure (columns must be multiple of 75)\n",
    "    if (len(read_file.columns)-2) % 75 == 0 and (len(read_file.columns)-2) // 75 > 0:\n",
    "        # File has proper structure - calculate maximum number of steps available\n",
    "        MAXstepCount = (len(read_file.columns)-2) // 75  # Each step = 75 columns of data\n",
    "        \n",
    "        # Extract data for each individual step\n",
    "        for stepcount in range(0, MAXstepCount):\n",
    "            # Extract IMU sensor data for this step (63 columns per step)\n",
    "            # Formula: [column_index * total_steps + current_step + offset]\n",
    "            DFperStep_imu = read_file.iloc[:,[idx*MAXstepCount+stepcount+Num_column_start for idx in range(0,63)]]\n",
    "            \n",
    "            # Extract biomechanics/spatial-temporal data for this step (12 columns per step)\n",
    "            # This data starts after all IMU columns and has different indexing pattern\n",
    "            DFperStep_SD = read_file.iloc[:, [(63*MAXstepCount)+idx+stepcount*3+Num_column_start for idx in range(0,12*(MAXstepCount)) if idx%(3*MAXstepCount) <3]]\n",
    "            \n",
    "            # Combine IMU and biomechanics data, remove completely empty rows\n",
    "            DFperStep = pd.concat([DFperStep_imu, DFperStep_SD], axis=1).dropna(how='all')\n",
    "            \n",
    "            # Create filename for individual step data\n",
    "            nametoSave = datum.split(\".\")[0] + \"_\" + str(stepcount+1) + \"_Step.csv\"\n",
    "            \n",
    "            # Save individual step data to include directory\n",
    "            DFperStep.to_csv(os.path.join(includeDir, nametoSave), index=False)\n",
    "        \n",
    "        # Record how many steps were successfully extracted from this file\n",
    "        excluded_len.append(stepcount+1)\n",
    "    else:\n",
    "        # File doesn't meet quality standards - move to exclude directory\n",
    "        read_file.to_csv(os.path.join(excludeDir, datum), index=False)\n",
    "        excluded_len.append(0)  # 0 indicates file was excluded\n",
    "\n",
    "# Create summary report of processing results\n",
    "listofname = pd.DataFrame(dataList, columns=[\"fileName\"])\n",
    "listofExcluded = pd.DataFrame(excluded_len, columns=[\"No. of Included\"])\n",
    "result = pd.concat([listofname, listofExcluded], axis=1)\n",
    "\n",
    "# Calculate statistics\n",
    "numofexcluded = (listofExcluded[listofExcluded.columns[0]] == 0).sum()  # Files completely excluded\n",
    "numofonestep = (listofExcluded[listofExcluded.columns[0]] == 1).sum()   # Files with 1 step extracted\n",
    "numoftwostep = (listofExcluded[listofExcluded.columns[0]] == 2).sum()   # Files with 2 steps extracted\n",
    "\n",
    "# Save processing summary to Excel file\n",
    "result.to_excel(os.path.join(dataDir, \"list_Excluded_DataByStep.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. Excluded: 391 \n",
      "No. 1 step Included: 828 \n",
      "No. 2 step Included: 85 \n",
      "No. total(verify): 1304 \n",
      "No. total(trueval): 1304\n",
      "==========================\n",
      "INC_ByStep contains 998 files = No. 1 step included + 2 * No. 2 step included(s)\n"
     ]
    }
   ],
   "source": [
    "# Data verification summary\n",
    "Numtotal = numofexcluded+numofonestep+numoftwostep\n",
    "print(f\"No. Excluded: {numofexcluded} \\nNo. 1 step Included: {numofonestep} \\nNo. 2 step Included: {numoftwostep} \\nNo. total(verify): {Numtotal} \\nNo. total(trueval): {len(dataList)}\")\n",
    "print(\"==========================\")\n",
    "print(f\"INC_ByStep contains {numofonestep + 2*numoftwostep} files = No. 1 step included + 2 * No. 2 step included(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclude columns filled with only zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1022b77f344643feae3de982d9439d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter out step-wise data that contains columns with only zeros\n",
    "# Reference: https://stackoverflow.com/questions/21164910/how-do-i-delete-a-column-that-contains-only-zeros-in-pandas\n",
    "\n",
    "# Define directory paths for zero-column filtering process\n",
    "dataDir =       r'R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\INC_ByStep'\n",
    "rawDir =       r'R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\INC_ByStep\\RAW'\n",
    "includeDir =    r'R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\INC_ByStep\\INC_ByZero\\RAW'\n",
    "excludeDir =    r'R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\INC_ByStep\\EXC'\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "ensure_dir(includeDir)\n",
    "ensure_dir(excludeDir)\n",
    "dataExt = r\".csv\"\n",
    "\n",
    "# Get list of all CSV files from the step-classified directory\n",
    "dataList = natsorted([_ for _ in os.listdir(rawDir) if _.endswith(dataExt)])\n",
    "excluded_zero = []  # Track which files pass/fail zero-column test (1=pass, 0=fail)\n",
    "\n",
    "# Process each file to identify and remove zero-only columns\n",
    "for datum in tqdm(dataList):\n",
    "    # Read individual step file\n",
    "    read_file = pd.read_csv(os.path.join(rawDir, datum))\n",
    "    \n",
    "    # Create working copy starting from row 4 (skip header/metadata rows)\n",
    "    tmp = read_file.iloc[4:,:].copy()\n",
    "    \n",
    "    # Convert all data to float for numerical analysis\n",
    "    tmp = tmp.astype(float)\n",
    "    \n",
    "    # Remove columns that contain only zeros\n",
    "    # (tmp != 0).any(axis=0) returns True for columns with at least one non-zero value\n",
    "    tmp = tmp.loc[:,(tmp != 0).any(axis=0)]\n",
    "    \n",
    "    # Check if remaining data still has valid structure (multiple of 75 columns)\n",
    "    if (len(tmp.columns)) % 75 == 0 and (len(tmp.columns)) // 75 > 0:\n",
    "        # Data is still valid after removing zero columns - save to include directory\n",
    "        read_file.to_csv(os.path.join(includeDir, datum), index=False)\n",
    "        excluded_zero.append(1)  # Mark as included\n",
    "    else:\n",
    "        # Removing zero columns broke the data structure - exclude this file\n",
    "        read_file.to_csv(os.path.join(excludeDir, datum), index=False)\n",
    "        excluded_zero.append(0)  # Mark as excluded\n",
    "\n",
    "# Create summary report of zero-column filtering results\n",
    "listofname = pd.DataFrame(dataList, columns=[\"fileName\"])\n",
    "listofExcluded = pd.DataFrame(excluded_zero, columns=[\"No. of Included\"])\n",
    "\n",
    "# Calculate statistics\n",
    "numofexcluded = (listofExcluded[listofExcluded.columns[0]] == 0).sum()  # Files excluded due to zero columns\n",
    "numofincluded = (listofExcluded[listofExcluded.columns[0]] == 1).sum()  # Files that passed zero-column test\n",
    "\n",
    "# Save filtering results to Excel file\n",
    "result = pd.concat([listofname, listofExcluded], axis=1)\n",
    "result.to_excel(os.path.join(dataDir, \"list_Excluded_byZero.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. Excluded: 95 \n",
      "No.Included: 903 \n",
      "No. total(verify): 998 \n",
      "No. total(truevl): 998\n",
      "==========================\n",
      "includ folder contain 903 file(s)\n"
     ]
    }
   ],
   "source": [
    "Numtotal = numofexcluded+numofincluded\n",
    "print(f\"No. Excluded: {numofexcluded} \\nNo.Included: {numofincluded} \\nNo. total(verify): {Numtotal} \\nNo. total(truevl): {len(dataList)}\")\n",
    "print(\"==========================\")\n",
    "print(f\"includ folder contain {numofincluded} file(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Axis conversion based on which foot was stepped\n",
    "# - oaleg - R : Right foot moment measurement\n",
    "# - oaleg - L : Left foot moment measurement  \n",
    "# - nonleg- R : Left foot moment measurement\n",
    "# - nonleg- L : Right foot moment measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDir =        r'R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\INC_ByStep\\INC_ByZero\\RAW'\n",
    "targetDir =    r'R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\INC_ByStep\\INC_ByZero\\RAW_AXIS_corrected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlegisLeft(sideInfo):\n",
    "    if (sideInfo == 'R'):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58adf0b176214d7f902b84fd79056a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/903 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ensure_dir(targetDir)\n",
    "dataExt = r\".csv\"\n",
    "\n",
    "# Get list of all CSV files from the directory\n",
    "dataList = natsorted([_ for _ in os.listdir(rawDir) if _.endswith(dataExt)])\n",
    "excluded_zero = []\n",
    "\n",
    "for datum in tqdm(dataList):\n",
    "    # Read individual file\n",
    "    read_file = pd.read_csv(os.path.join(rawDir, datum))\n",
    "    \n",
    "    # Convert data rows (starting from row 4) to float for numerical processing\n",
    "    read_file.iloc[4:,:] = read_file.iloc[4:,:].astype(float)\n",
    "    \n",
    "    # Extract side and leg information from filename\n",
    "    sideInfo = datum.split('_')[-3]  # Get affected side info\n",
    "    legInfo = datum.split('_')[4]    # Get leg info\n",
    "    \n",
    "    # If nonleg is the left foot\n",
    "    if nonlegisLeft(sideInfo):\n",
    "        # Define target columns for axis conversion (skip certain column ranges)\n",
    "        # Excludes columns where (col%9 >0 and col%9<4) or (col%9 >5 and col%9<9)\n",
    "        targetcol = [_ for _ in range(0,27) if not ((_%9 >0) and (_%9<4)) or ((_%9 >5) and (_%9<9))]\n",
    "        \n",
    "        # Apply negative transformation to selected columns\n",
    "        for col in targetcol:\n",
    "            read_file.iloc[4:,col] = read_file.iloc[4:,col].apply(lambda x:x*-1)\n",
    "        # Modify nonleg data!\n",
    "    \n",
    "    # If nonleg is not the left foot (i.e., nonleg is right foot)\n",
    "    else:\n",
    "        # Define target columns with offset of 27 for the other leg's data\n",
    "        targetcol = [_ + 27 for _ in range(0,27) if not ((_%9 >0) and (_%9<4)) or ((_%9 >5) and (_%9<9))]\n",
    "        \n",
    "        # Apply negative transformation to selected columns\n",
    "        for col in targetcol:\n",
    "            read_file.iloc[4:,col] = read_file.iloc[4:,col].apply(lambda x:x*-1)\n",
    "        # Modify oaleg data!\n",
    "    \n",
    "    # Save the processed file with axis corrections\n",
    "    read_file.to_csv(os.path.join(targetDir,datum),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF plotting for visual inspection\n",
    "# - Generate PDF plots for manual visual verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary functions\n",
    "def axis3plot(position, data, name):\n",
    "    \"\"\"\n",
    "    Creates a 3-axis plot with X, Y, Z data on separate y-axes\n",
    "    Args:\n",
    "        position: subplot position for matplotlib\n",
    "        data: DataFrame with 3 columns (X, Y, Z axis data)\n",
    "        name: label for the plot/sensor type\n",
    "    \"\"\"\n",
    "    # Plot layout parameters\n",
    "    subAJ_right = 0.5      # Right margin adjustment\n",
    "    subAJ_top = 1.5        # Top margin adjustment  \n",
    "    subAJ_wspace = 0.5     # Width spacing between subplots\n",
    "    sub_offset = 40        # Offset for third y-axis\n",
    "    \n",
    "    # Convert data to float for plotting\n",
    "    data = data.astype(float)\n",
    "    \n",
    "    # Create main plot host with custom axes\n",
    "    host = host_subplot(position, axes_class=AA.Axes)\n",
    "    plt.subplots_adjust(right=subAJ_right, top=subAJ_top, wspace=subAJ_wspace)\n",
    "    \n",
    "    # Create two additional y-axes (twin axes) for Y and Z data\n",
    "    par1 = host.twinx()  # Second y-axis for Y data\n",
    "    par2 = host.twinx()  # Third y-axis for Z data\n",
    "    \n",
    "    # Position the third y-axis with an offset to avoid overlap\n",
    "    offset = sub_offset\n",
    "    new_fixed_axis = par2.get_grid_helper().new_fixed_axis\n",
    "    par2.axis[\"right\"] = new_fixed_axis(loc=\"right\",\n",
    "                                        axes=par2,\n",
    "                                        offset=(offset, 0))\n",
    "    \n",
    "    # Enable visibility for the right-side y-axes\n",
    "    par1.axis[\"right\"].toggle(all=True) \n",
    "    par2.axis[\"right\"].toggle(all=True)\n",
    "    \n",
    "    # Set axis labels\n",
    "    host.set_xlabel(f\"Time/{name}\")  # X-axis shows time with sensor name\n",
    "    \n",
    "    # Plot the three data series (X, Y, Z) on their respective axes\n",
    "    p1, = host.plot(range(0, len(data)), np.array(data.iloc[:,0]), label=\"X_axis\")  # X data on main axis\n",
    "    p2, = par1.plot(range(0, len(data)), np.array(data.iloc[:,1]), label=\"Y_axis\")  # Y data on second axis\n",
    "    p3, = par2.plot(range(0, len(data)), np.array(data.iloc[:,2]), label=\"Z_axis\")  # Z data on third axis\n",
    "    \n",
    "    # Add legend to identify the three lines\n",
    "    host.legend()\n",
    "    \n",
    "    # Color-code the y-axis labels, ticks, and tick labels to match their data lines\n",
    "    host.axis[\"left\"].label.set_color(p1.get_color())           # X-axis (left) matches X data color\n",
    "    par1.axis[\"right\"].label.set_color(p2.get_color())         # Y-axis (right) matches Y data color  \n",
    "    par2.axis[\"right\"].label.set_color(p3.get_color())         # Z-axis (offset right) matches Z data color\n",
    "    \n",
    "    host.axis[\"left\"].major_ticks.set_color(p1.get_color())     # Color the tick marks\n",
    "    par1.axis[\"right\"].major_ticks.set_color(p2.get_color())\n",
    "    par2.axis[\"right\"].major_ticks.set_color(p3.get_color())\n",
    "    \n",
    "    host.axis[\"left\"].major_ticklabels.set_color(p1.get_color()) # Color the tick labels\n",
    "    par1.axis[\"right\"].major_ticklabels.set_color(p2.get_color())\n",
    "    par2.axis[\"right\"].major_ticklabels.set_color(p3.get_color())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'R:\\\\KumarLab3\\\\PROJECTS\\\\wesens\\\\Data\\\\Analysis\\\\smith_dl\\\\IMU Deep Learning\\\\Data\\\\allnew_20220325_raw_byDeepak_csv\\\\INC_ByStep\\\\INC_ByZero\\\\AXIS_correction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m dataExt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Get list of all CSV files from the axis-corrected directory\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m dataList \u001b[38;5;241m=\u001b[39m natsorted([_ \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrawDir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m _\u001b[38;5;241m.\u001b[39mendswith(dataExt)])\n\u001b[0;32m     20\u001b[0m excluded_zero \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m datum \u001b[38;5;129;01min\u001b[39;00m tqdm(dataList):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Read data file\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'R:\\\\KumarLab3\\\\PROJECTS\\\\wesens\\\\Data\\\\Analysis\\\\smith_dl\\\\IMU Deep Learning\\\\Data\\\\allnew_20220325_raw_byDeepak_csv\\\\INC_ByStep\\\\INC_ByZero\\\\AXIS_correction'"
     ]
    }
   ],
   "source": [
    "# Generate images and visually assess data to determine what should be excluded\n",
    "# Here we only generate the images\n",
    "# Visual inspection and exclusion will be done separately in a .py script\n",
    "\n",
    "# Define necessary directories\n",
    "dataDir =       r'R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\INC_ByStep\\INC_ByZero'\n",
    "rawDir =        r'R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\INC_ByStep\\INC_ByZero\\AXIS_correction'\n",
    "figDir =        r'R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\INC_ByStep\\INC_ByZero\\FIG'\n",
    "includeDir =    r'R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\INC_ByStep\\INC_ByZero\\Included_checked\\RAW'\n",
    "excludeDir =    r'R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\INC_ByStep\\INC_ByZero\\EXC'\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "ensure_dir(figDir)\n",
    "ensure_dir(includeDir)\n",
    "ensure_dir(excludeDir)\n",
    "dataExt = r\".csv\"\n",
    "\n",
    "# Get list of all CSV files from the axis-corrected directory\n",
    "dataList = natsorted([_ for _ in os.listdir(rawDir) if _.endswith(dataExt)])\n",
    "excluded_zero = []\n",
    "\n",
    "for datum in tqdm(dataList):\n",
    "    # Read data file\n",
    "    read_file = pd.read_csv(os.path.join(rawDir, datum))\n",
    "    \n",
    "    # Generate figure only if PDF doesn't already exist\n",
    "    if not os.path.exists(os.path.join(figDir, f'{datum.split(\".\")[0]}.pdf')):\n",
    "        pp = PdfPages(os.path.join(figDir, f'{datum.split(\".\")[0]}.pdf'))\n",
    "        \n",
    "        ################################# Page 1 ###############\n",
    "        # Plot first 9 sensor groups (columns 0-26)\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        for i in range(0, 9):\n",
    "            axis3plot(331+i, read_file.iloc[4:, 3*i:3+i*3], '_'.join(read_file.iloc[0, 3*i:3+i*3][0].split('_')[:-1]))\n",
    "        plt.tight_layout()\n",
    "        pp.savefig()\n",
    "        plt.close()\n",
    "        \n",
    "        ################################# Page 2 ###############\n",
    "        # Starting column position for second leg data\n",
    "        col_startForFig = 27\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        for i in range(0, 9):\n",
    "            axis3plot(331+i, read_file.iloc[4:, col_startForFig+3*i:col_startForFig+3+i*3], '_'.join(read_file.iloc[0, col_startForFig+3*i:col_startForFig+3+i*3][0].split('_')[:-1]))\n",
    "        plt.tight_layout()\n",
    "        pp.savefig()\n",
    "        plt.close()\n",
    "        \n",
    "        ################################# Page 3 ###############\n",
    "        # Starting column position for additional sensor data\n",
    "        col_startForFig = 54\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        for i in range(0, 3):\n",
    "            axis3plot(331+i, read_file.iloc[4:, col_startForFig+3*i:col_startForFig+3+i*3], '_'.join(read_file.iloc[0, col_startForFig+3*i:col_startForFig+3+i*3][0].split('_')[:-1]))\n",
    "        plt.tight_layout()\n",
    "        pp.savefig()\n",
    "        plt.close()\n",
    "        \n",
    "        ################################# Page 4 ###############\n",
    "        col_startForFig = 63\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        for i in range(0, 4):\n",
    "            axis3plot(331+i, read_file.iloc[4:, col_startForFig+3*i:col_startForFig+3+i*3], '_'.join(read_file.iloc[0, col_startForFig+3*i:col_startForFig+3+i*3][0].split('_')[:-1]))\n",
    "        plt.tight_layout()\n",
    "        pp.savefig()\n",
    "        plt.close()\n",
    "        \n",
    "        ############################ After all plots are generated ############################\n",
    "        pp.close()\n",
    "        \n",
    "        # Note: Creating 903 PDFs takes about 2 hours - is this correct?\n",
    "        # Processing speed is fast, but reading/writing data over network seems slow\n",
    "        # About 3 seconds per file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 끝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is there a size difference between the original data (908 files) and the newly created data (903 files)?\n",
    "- The data count of 903 files is correct\n",
    "- But the original dataset contains 5 extra files\n",
    "- Let's find the culprit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to compare PDF files between new and original datasets\n",
    "allnew = r'R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\allnew_20220325_raw_byDeepak_csv\\INC_ByStep\\INC_ByZero\\FIG'\n",
    "origin = r'R:\\KumarLab3\\PROJECTS\\wesens\\Data\\Analysis\\smith_dl\\IMU Deep Learning\\Data\\20220325_raw_byDeepak_csv\\INC_ByStep\\INC_ByZero\\FIG'\n",
    "dataExt = r\".pdf\"\n",
    "\n",
    "# Get complete file lists and filter for PDF files only\n",
    "allnewdataList = natsorted([_ for _ in os.listdir(allnew) if _.endswith(dataExt)])  # New dataset PDF files\n",
    "origindataList = natsorted([_ for _ in os.listdir(origin) if _.endswith(dataExt)])  # Original dataset PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process filename to remove step information for comparison\n",
    "# Take the first filename and split it by underscores\n",
    "str_name = np.array(allnewdataList[0].split('_'))\n",
    "\n",
    "# Remove the third-to-last element (likely the step number like \"1\" or \"2\")\n",
    "str_name = np.delete(str_name, -3)\n",
    "\n",
    "# Rejoin the parts back into a filename string\n",
    "str_name = '_'.join(str_name)\n",
    "\n",
    "# Display the processed filename\n",
    "str_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all filenames to create standardized names for comparison\n",
    "renamed = []\n",
    "for data in allnewdataList:\n",
    "    # Split filename by underscores into array\n",
    "    str_name = np.array(data.split('_'))\n",
    "    \n",
    "    # Remove the third-to-last element (step number) from the filename\n",
    "    str_name = np.delete(str_name, -3)\n",
    "    \n",
    "    # Rejoin the parts back into a standardized filename\n",
    "    str_name = '_'.join(str_name)\n",
    "    \n",
    "    # Add the processed filename to the list\n",
    "    renamed.append(str_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find files that exist in one dataset but not the other (symmetric difference)\n",
    "# This will show files that are only in the original dataset OR only in the new dataset\n",
    "set(origindataList) ^ set(renamed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary - Newly learned code this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Scalable code!!\n",
    "## We did it!\n",
    "\n",
    "# Configuration parameters\n",
    "Num_column_start = 2    # Number of columns to skip at the beginning (empty columns)\n",
    "MAXstepCount = 3        # Maximum number of steps in the data\n",
    "stepcount = 2           # Current step to extract (0-indexed, so this is step 3)\n",
    "\n",
    "# Complex column indexing formula for extracting biomechanics data for a specific step\n",
    "# This extracts spatial-temporal/biomechanics data (12 columns per step) for the specified step\n",
    "[(63*MAXstepCount) + idx + stepcount*3 + Num_column_start \n",
    " for idx in range(0, 12*(MAXstepCount)) \n",
    " if idx%(3*MAXstepCount) < 3]\n",
    "\n",
    "# Breakdown of the formula:\n",
    "# - (63*MAXstepCount): Skip all IMU data columns (63 columns × number of steps)\n",
    "# - idx + stepcount*3: Navigate to the correct step's data within the biomechanics section\n",
    "# - Num_column_start: Account for initial empty columns\n",
    "# - idx%(3*MAXstepCount) < 3: Filter condition to select only the first 3 columns of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of creating multiple 3-axis plots in a grid layout\n",
    "# Reference: https://matplotlib.org/3.5.0/gallery/axisartist/demo_parasite_axes2.html\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import mpl_toolkits.axisartist as AA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Plot layout parameters\n",
    "subAJ_right = 1.5      # Right margin adjustment\n",
    "subAJ_top = 1.5        # Top margin adjustment  \n",
    "subAJ_wspace = 0.4     # Width spacing between subplots\n",
    "sub_offset = 15        # Offset for third y-axis\n",
    "\n",
    "# =============== First subplot (position 331) ===============\n",
    "host = host_subplot(331, axes_class=AA.Axes)\n",
    "plt.subplots_adjust(right=subAJ_right, top=subAJ_top, wspace=subAJ_wspace)\n",
    "\n",
    "# Create twin axes for Y and Z data\n",
    "par1 = host.twinx()  # Second y-axis\n",
    "par2 = host.twinx()  # Third y-axis\n",
    "\n",
    "# Position the third y-axis with offset to avoid overlap\n",
    "offset = sub_offset\n",
    "new_fixed_axis = par2.get_grid_helper().new_fixed_axis\n",
    "par2.axis[\"right\"] = new_fixed_axis(loc=\"right\", axes=par2, offset=(offset, 0))\n",
    "\n",
    "# Enable visibility for right-side axes\n",
    "par1.axis[\"right\"].toggle(all=True) \n",
    "par2.axis[\"right\"].toggle(all=True)\n",
    "\n",
    "# Set labels and plot sample data\n",
    "host.set_xlabel(\"Time\")\n",
    "p1, = host.plot([0, 1, 2], [0, 1, 2], label=\"X_axis\")     # X data on main axis\n",
    "p2, = par1.plot([0, 1, 2], [0, 3, 2], label=\"Y_axis\")    # Y data on second axis\n",
    "p3, = par2.plot([0, 1, 2], [50, 30, 15], label=\"Z_axis\") # Z data on third axis\n",
    "\n",
    "# Add legend\n",
    "host.legend()\n",
    "\n",
    "# Color-code axes to match their data lines\n",
    "host.axis[\"left\"].label.set_color(p1.get_color())\n",
    "par1.axis[\"right\"].label.set_color(p2.get_color())\n",
    "par2.axis[\"right\"].label.set_color(p3.get_color())\n",
    "\n",
    "host.axis[\"left\"].major_ticks.set_color(p1.get_color())\n",
    "par1.axis[\"right\"].major_ticks.set_color(p2.get_color())\n",
    "par2.axis[\"right\"].major_ticks.set_color(p3.get_color())\n",
    "\n",
    "host.axis[\"left\"].major_ticklabels.set_color(p1.get_color())\n",
    "par1.axis[\"right\"].major_ticklabels.set_color(p2.get_color())\n",
    "par2.axis[\"right\"].major_ticklabels.set_color(p3.get_color())\n",
    "\n",
    "# =============== Second subplot (position 332) ===============\n",
    "host = host_subplot(332, axes_class=AA.Axes)\n",
    "plt.subplots_adjust(right=subAJ_right, top=subAJ_top, wspace=subAJ_wspace)\n",
    "\n",
    "# Create twin axes\n",
    "par1 = host.twinx()\n",
    "par2 = host.twinx()\n",
    "\n",
    "# Position third y-axis\n",
    "offset = sub_offset\n",
    "new_fixed_axis = par2.get_grid_helper().new_fixed_axis\n",
    "par2.axis[\"right\"] = new_fixed_axis(loc=\"right\", axes=par2, offset=(offset, 0))\n",
    "\n",
    "par1.axis[\"right\"].toggle(all=True) \n",
    "par2.axis[\"right\"].toggle(all=True)\n",
    "\n",
    "# Plot different sample data for this subplot\n",
    "host.set_xlabel(\"Time\")\n",
    "p1, = host.plot([0, 1, 2], [0, 2, 1], label=\"X_axis\")\n",
    "p2, = par1.plot([0, 1, 2], [0, 5, 2], label=\"Y_axis\")\n",
    "p3, = par2.plot([0, 1, 2], [50, 2, 15], label=\"Z_axis\")\n",
    "\n",
    "host.legend()\n",
    "\n",
    "# Color-code axes\n",
    "host.axis[\"left\"].label.set_color(p1.get_color())\n",
    "par1.axis[\"right\"].label.set_color(p2.get_color())\n",
    "par2.axis[\"right\"].label.set_color(p3.get_color())\n",
    "\n",
    "host.axis[\"left\"].major_ticks.set_color(p1.get_color())\n",
    "par1.axis[\"right\"].major_ticks.set_color(p2.get_color())\n",
    "par2.axis[\"right\"].major_ticks.set_color(p3.get_color())\n",
    "\n",
    "host.axis[\"left\"].major_ticklabels.set_color(p1.get_color())\n",
    "par1.axis[\"right\"].major_ticklabels.set_color(p2.get_color())\n",
    "par2.axis[\"right\"].major_ticklabels.set_color(p3.get_color())\n",
    "\n",
    "# =============== Third subplot (position 333) ===============\n",
    "host = host_subplot(333, axes_class=AA.Axes)\n",
    "plt.subplots_adjust(right=subAJ_right, top=subAJ_top, wspace=subAJ_wspace)\n",
    "\n",
    "par1 = host.twinx()\n",
    "par2 = host.twinx()\n",
    "\n",
    "offset = sub_offset\n",
    "new_fixed_axis = par2.get_grid_helper().new_fixed_axis\n",
    "par2.axis[\"right\"] = new_fixed_axis(loc=\"right\", axes=par2, offset=(offset, 0))\n",
    "\n",
    "par1.axis[\"right\"].toggle(all=True) \n",
    "par2.axis[\"right\"].toggle(all=True)\n",
    "\n",
    "host.set_xlabel(\"Time\")\n",
    "p1, = host.plot([0, 1, 2], [0, 1, 2], label=\"X_axis\")\n",
    "p2, = par1.plot([0, 1, 2], [0, 3, 2], label=\"Y_axis\")\n",
    "p3, = par2.plot([0, 1, 2], [50, 30, 15], label=\"Z_axis\")\n",
    "\n",
    "host.legend()\n",
    "\n",
    "# Color-code axes\n",
    "host.axis[\"left\"].label.set_color(p1.get_color())\n",
    "par1.axis[\"right\"].label.set_color(p2.get_color())\n",
    "par2.axis[\"right\"].label.set_color(p3.get_color())\n",
    "\n",
    "host.axis[\"left\"].major_ticks.set_color(p1.get_color())\n",
    "par1.axis[\"right\"].major_ticks.set_color(p2.get_color())\n",
    "par2.axis[\"right\"].major_ticks.set_color(p3.get_color())\n",
    "\n",
    "host.axis[\"left\"].major_ticklabels.set_color(p1.get_color())\n",
    "par1.axis[\"right\"].major_ticklabels.set_color(p2.get_color())\n",
    "par2.axis[\"right\"].major_ticklabels.set_color(p3.get_color())\n",
    "\n",
    "# =============== Fourth subplot (position 334) ===============\n",
    "host = host_subplot(334, axes_class=AA.Axes)\n",
    "plt.subplots_adjust(right=subAJ_right, top=subAJ_top, wspace=subAJ_wspace)\n",
    "\n",
    "par1 = host.twinx()\n",
    "par2 = host.twinx()\n",
    "\n",
    "offset = sub_offset\n",
    "new_fixed_axis = par2.get_grid_helper().new_fixed_axis\n",
    "par2.axis[\"right\"] = new_fixed_axis(loc=\"right\", axes=par2, offset=(offset, 0))\n",
    "\n",
    "par1.axis[\"right\"].toggle(all=True) \n",
    "par2.axis[\"right\"].toggle(all=True)\n",
    "\n",
    "host.set_xlabel(\"Time\")\n",
    "p1, = host.plot([0, 1, 2], [0, 2, 1], label=\"X_axis\")\n",
    "p2, = par1.plot([0, 1, 2], [0, 5, 2], label=\"Y_axis\")\n",
    "p3, = par2.plot([0, 1, 2], [50, 2, 15], label=\"Z_axis\")\n",
    "\n",
    "host.legend()\n",
    "\n",
    "# Color-code axes\n",
    "host.axis[\"left\"].label.set_color(p1.get_color())\n",
    "par1.axis[\"right\"].label.set_color(p2.get_color())\n",
    "par2.axis[\"right\"].label.set_color(p3.get_color())\n",
    "\n",
    "host.axis[\"left\"].major_ticks.set_color(p1.get_color())\n",
    "par1.axis[\"right\"].major_ticks.set_color(p2.get_color())\n",
    "par2.axis[\"right\"].major_ticks.set_color(p3.get_color())\n",
    "\n",
    "host.axis[\"left\"].major_ticklabels.set_color(p1.get_color())\n",
    "par1.axis[\"right\"].major_ticklabels.set_color(p2.get_color())\n",
    "par2.axis[\"right\"].major_ticklabels.set_color(p3.get_color())\n",
    "\n",
    "# Finalize layout and save to PDF\n",
    "plt.tight_layout()\n",
    "\n",
    "result_fig = plt.draw()\n",
    "# Save the complete figure to PDF\n",
    "pp = PdfPages(os.path.join(r'Z:\\PROJECTS\\iwalqq\\Data\\V3D\\Output\\IMU Deep Learning\\Data\\20220325_raw_byDeepak_csv\\INC_ByStep\\INC_ByZero\\FIG','line_plot.pdf'))\n",
    "pp.savefig(result_fig)\n",
    "pp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sinusoidal sample data\n",
    "sample_length = range(1, 15+1)\n",
    "rads = np.arange(0, 2*np.pi, 0.01)\n",
    "data = np.array([np.sin(t*rads) for t in sample_length])\n",
    "df = pd.DataFrame(data.T, index=pd.Series(rads.tolist(), name='radians'), columns=[f'freq: {i}x' for i in sample_length])\n",
    "\n",
    "\n",
    "# default plot with subplots; each column is a subplot\n",
    "axes = df.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = df.plot(subplots=True, layout=(3, 5), figsize=(25, 16), sharex=True, sharey=True)\n",
    "\n",
    "# flatten the axes array to easily access any subplot\n",
    "axes = axes.flat\n",
    "\n",
    "# extract the figure object\n",
    "fig = axes[0].get_figure()\n",
    "\n",
    "# use tight_layout\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수화\n",
    "def axis3plot(position,data,name):\n",
    "    subAJ_right = 0.5\n",
    "    subAJ_top = 1.5\n",
    "    subAJ_wspace = 0.5\n",
    "    sub_offset= 40\n",
    "\n",
    "    data = data.astype(float)\n",
    "    \n",
    "    host = host_subplot(position, axes_class=AA.Axes)\n",
    "    plt.subplots_adjust(right=subAJ_right,top=subAJ_top, wspace=subAJ_wspace)\n",
    "\n",
    "    par1 = host.twinx()\n",
    "    par2 = host.twinx()\n",
    "\n",
    "    offset = sub_offset\n",
    "    new_fixed_axis = par2.get_grid_helper().new_fixed_axis\n",
    "    par2.axis[\"right\"] = new_fixed_axis(loc=\"right\",\n",
    "                                        axes=par2,\n",
    "                                        offset=(offset, 0))\n",
    "\n",
    "    par1.axis[\"right\"].toggle(all=True) \n",
    "    par2.axis[\"right\"].toggle(all=True)\n",
    "\n",
    "    # host.set_xlim(0, 2)\n",
    "    # host.set_ylim(0, 2)\n",
    "\n",
    "    host.set_xlabel(f\"Time/{name}\")\n",
    "    # host.set_ylabel(\"X_axis\")\n",
    "    # par1.set_ylabel(\"Y_axis\")\n",
    "    # par2.set_ylabel(\"Z_axis\")\n",
    "\n",
    "    p1, = host.plot(range(0,len(data)), np.array(data.iloc[:,0]), label=\"X_axis\")\n",
    "    p2, = par1.plot(range(0,len(data)),  np.array(data.iloc[:,1]), label=\"Y_axis\")\n",
    "    p3, = par2.plot(range(0,len(data)),  np.array(data.iloc[:,2]), label=\"Z_axis\")\n",
    "\n",
    "    # par1.set_ylim(0, 4)\n",
    "    # par2.set_ylim(1, 65)\n",
    "\n",
    "    host.legend()\n",
    "\n",
    "    host.axis[\"left\"].label.set_color(p1.get_color())\n",
    "    par1.axis[\"right\"].label.set_color(p2.get_color())\n",
    "    par2.axis[\"right\"].label.set_color(p3.get_color())\n",
    "\n",
    "    host.axis[\"left\"].major_ticks.set_color(p1.get_color())\n",
    "    par1.axis[\"right\"].major_ticks.set_color(p2.get_color())\n",
    "    par2.axis[\"right\"].major_ticks.set_color(p3.get_color())\n",
    "\n",
    "    host.axis[\"left\"].major_ticklabels.set_color(p1.get_color())\n",
    "    par1.axis[\"right\"].major_ticklabels.set_color(p2.get_color())\n",
    "    par2.axis[\"right\"].major_ticklabels.set_color(p3.get_color())\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "axis3plot(331,non_shank_acc,'non_shank_acc')\n",
    "axis3plot(332,non_shank_acc,'non_shank_acc')\n",
    "axis3plot(333,non_shank_acc,'non_shank_acc')\n",
    "axis3plot(334,non_shank_acc,'non_shank_acc')\n",
    "axis3plot(335,non_shank_acc,'non_shank_acc')\n",
    "axis3plot(336,non_shank_acc,'non_shank_acc')\n",
    "axis3plot(338,non_shank_acc,'non_shank_acc')\n",
    "plt.tight_layout()\n",
    "\n",
    "ressult_fig = plt.draw()\n",
    "# plt.show()\n",
    "pp = PdfPages(os.path.join(r'Z:\\PROJECTS\\iwalqq\\Data\\V3D\\Output\\IMU Deep Learning\\Data\\20220325_raw_byDeepak_csv\\INC_ByStep\\INC_ByZero\\FIG','line_plot.pdf'))\n",
    "pp.savefig(ressult_fig)\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "A = 'W002_20210715_isok_concon60_0001_Miqus_21_25762.avi'\n",
    "p = re.compile(r'_([0-9]{5})\\.')\n",
    "p.search(A)[0][1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "deaecf69fdd75db6910ee7dbdef1256191e6e0ba8ec080482b87a38c123d274f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
